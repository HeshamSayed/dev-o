Advanced Multi-Agent System with Local LLM
Let me explain the key concepts and architecture for building this advanced system, then give you a comprehensive prompt.

1. Understanding DeepSeek-R1 Locally
What is DeepSeek-R1?
DeepSeek-R1 is a reasoning model (like OpenAI's o1) that "thinks" before answering. The 7B version can run on your machine with ~8GB VRAM.
Running with Ollama
bash# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Pull DeepSeek-R1 7B
ollama pull deepseek-r1:7b

# Test it
ollama run deepseek-r1:7b "Write a Python function to calculate factorial"
```

### Key Considerations for Local LLM
- **Context Window**: DeepSeek-R1:7b has ~32K context (smaller than GPT-4's 128K)
- **Speed**: Slower than API calls, plan for async execution
- **Reasoning Tokens**: R1 models output "thinking" tokens, you need to parse them
- **Memory**: Need smart chunking since context is limited

---

## 2. Advanced Context Sharing Architecture

This is the **most critical** part for agents to work together without bugs.

### The Problem
Agent A creates a user model. Agent B needs to create an API for users. How does B know exactly what A created?

### Solution: Multi-Layer Context System
```
┌─────────────────────────────────────────────────────────────────┐
│                    CONTEXT LAYERS                               │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Layer 1: PROJECT MANIFEST (Always loaded)                      │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │ - Project requirements (structured)                      │   │
│  │ - Tech stack decisions                                   │   │
│  │ - Architecture overview                                  │   │
│  │ - Entity/Domain model                                    │   │
│  │ - API contracts (OpenAPI spec)                          │   │
│  │ - Database schema                                        │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  Layer 2: SHARED STATE (Real-time sync)                        │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │ - Current file tree                                      │   │
│  │ - All agents' status & current tasks                     │   │
│  │ - Pending decisions awaiting resolution                  │   │
│  │ - Blockers and dependencies                              │   │
│  │ - Recent changes (last 1 hour)                          │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  Layer 3: SEMANTIC MEMORY (Retrieved on demand)                │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │ - Code chunks with embeddings                            │   │
│  │ - Past decisions and reasoning                           │   │
│  │ - Completed task summaries                               │   │
│  │ - Error resolutions                                      │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  Layer 4: AGENT WORKING MEMORY (Per agent)                     │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │ - Current task details                                   │   │
│  │ - Files being modified                                   │   │
│  │ - Conversation history                                   │   │
│  │ - Tool call results                                      │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
The "Artifact Registry" Pattern
Every time an agent creates something, it's registered:
python# When backend agent creates a model
artifact = {
    "type": "model",
    "name": "User",
    "file": "apps/users/models.py",
    "fields": ["id", "email", "password_hash", "created_at"],
    "relationships": ["has_many: Order"],
    "created_by": "backend_lead_agent",
    "task_id": "task_123",
    "dependencies": [],
    "dependents": []  # Updated when other artifacts use this
}

# When API agent needs to create user endpoint
# System automatically includes User model artifact in context
Event Sourcing for Agent Actions
Every agent action is logged as an event:
pythonevents = [
    {"type": "file_created", "agent": "backend_lead", "file": "models.py", "timestamp": "..."},
    {"type": "file_modified", "agent": "backend_lead", "file": "models.py", "diff": "...", "timestamp": "..."},
    {"type": "decision_made", "agent": "architect", "decision": "Use JWT auth", "reasoning": "..."},
    {"type": "task_completed", "agent": "backend_senior", "task_id": "...", "summary": "..."},
]

# Any agent can replay events to understand what happened
```

---

## 3. Interactive CLI Design (Like Claude Code)

### Key Features Needed

1. **Streaming Output**: Show agent thinking in real-time
2. **Interrupt Handling**: Ctrl+C to pause, ask question, or cancel
3. **Multi-line Input**: For pasting code or long descriptions
4. **Context Persistence**: Resume where you left off
5. **Status Dashboard**: See all agents' status

### Interrupt Handling Pattern
```
User types message
    ↓
Agent starts processing (streaming)
    ↓
User presses Ctrl+C
    ↓
┌─────────────────────────────────┐
│ What would you like to do?      │
│                                 │
│ [1] Pause and ask a question    │
│ [2] Cancel current task         │
│ [3] Continue processing         │
│ [4] Save checkpoint and exit    │
└─────────────────────────────────┘
    ↓
If "Pause": Save agent state → Accept user input → Resume
If "Cancel": Rollback changes → Return to prompt
If "Save": Serialize full state → Exit gracefully
State Recovery
python# On every significant action, save checkpoint
checkpoint = {
    "project_id": "...",
    "agents": [
        {
            "id": "...",
            "type": "backend_lead",
            "status": "working",
            "current_task": "...",
            "working_memory": {...},
            "conversation": [...],
            "pending_tool_calls": [...]
        }
    ],
    "pending_changes": [
        {"file": "models.py", "content": "...", "status": "uncommitted"}
    ],
    "timestamp": "..."
}

# On resume
# 1. Load checkpoint
# 2. Verify file system matches expected state
# 3. Resume agent from last known state
```

---

## 4. Subscription System Design
```
┌─────────────────────────────────────────────────────────────────┐
│                    SUBSCRIPTION TIERS                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  FREE TIER                                                      │
│  - 1 project                                                    │
│  - Basic agents (1 backend, 1 frontend)                        │
│  - 100 agent actions/day                                       │
│  - Local LLM only                                              │
│                                                                 │
│  PRO TIER ($29/month)                                          │
│  - 5 projects                                                   │
│  - All agent types                                             │
│  - 1000 agent actions/day                                      │
│  - Local LLM + API fallback (GPT-4, Claude)                   │
│  - Priority support                                            │
│                                                                 │
│  TEAM TIER ($99/month)                                         │
│  - Unlimited projects                                          │
│  - Unlimited agent actions                                     │
│  - Team collaboration                                          │
│  - Custom agent training                                       │
│  - API access                                                  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘

5. Error Prevention & Continuation Strategy
Why Agents Make Errors

Missing Context: Agent doesn't know what another agent did
Stale Information: Code changed but agent has old version
Conflicting Decisions: Two agents make incompatible choices
Incomplete Tasks: Agent interrupted mid-task

Solutions
a) Pre-flight Checks
Before an agent acts, verify:
pythondef pre_flight_check(agent, action):
    checks = []
    
    # Check 1: Are dependencies satisfied?
    if action.requires_files:
        for file in action.requires_files:
            if not file_exists(file):
                checks.append(f"Missing dependency: {file}")
    
    # Check 2: Is the file locked by another agent?
    if action.modifies_file:
        lock = get_file_lock(action.file)
        if lock and lock.agent != agent:
            checks.append(f"File locked by {lock.agent}")
    
    # Check 3: Is the context fresh?
    if agent.context_age > 60:  # seconds
        checks.append("Context may be stale, refreshing...")
        refresh_agent_context(agent)
    
    return checks
b) Conflict Detection
pythondef detect_conflicts(pending_changes):
    conflicts = []
    
    # Check for overlapping file modifications
    files_modified = defaultdict(list)
    for change in pending_changes:
        files_modified[change.file].append(change)
    
    for file, changes in files_modified.items():
        if len(changes) > 1:
            conflicts.append({
                "type": "concurrent_modification",
                "file": file,
                "agents": [c.agent for c in changes]
            })
    
    # Check for breaking changes
    for change in pending_changes:
        dependents = get_dependents(change.file)
        for dep in dependents:
            if is_breaking_change(change, dep):
                conflicts.append({
                    "type": "breaking_change",
                    "change": change,
                    "affected": dep
                })
    
    return conflicts
c) Automatic Recovery
pythondef recover_from_error(agent, error, task):
    # Log the error
    log_error(agent, error, task)
    
    # Analyze error type
    if error.type == "syntax_error":
        # Ask agent to fix
        return agent.fix_code(error.file, error.message)
    
    elif error.type == "test_failure":
        # Provide test output, ask to fix
        return agent.fix_failing_test(error.test, error.output)
    
    elif error.type == "missing_dependency":
        # Find who should create it
        responsible_agent = find_responsible_agent(error.dependency)
        create_blocking_task(responsible_agent, error.dependency)
        mark_task_blocked(task, error.dependency)
    
    elif error.type == "conflict":
        # Escalate to lead or architect
        escalate_to_lead(agent, error)

6. LLM Integration Architecture
Ollama Client
pythonclass OllamaClient:
    def __init__(self, base_url="http://localhost:11434"):
        self.base_url = base_url
        self.model = "deepseek-r1:7b"
    
    async def generate(self, prompt, system=None, stream=True):
        """Generate with streaming support"""
        payload = {
            "model": self.model,
            "prompt": prompt,
            "system": system,
            "stream": stream,
            "options": {
                "temperature": 0.7,
                "num_ctx": 32768,  # Max context
            }
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/api/generate",
                json=payload
            ) as response:
                if stream:
                    async for line in response.content:
                        data = json.loads(line)
                        yield data.get("response", "")
                        if data.get("done"):
                            break
                else:
                    data = await response.json()
                    yield data["response"]
    
    def parse_reasoning(self, response):
        """Extract thinking and answer from R1 response"""
        # R1 outputs: <think>...</think> then answer
        if "<think>" in response:
            think_match = re.search(r"<think>(.*?)</think>", response, re.DOTALL)
            thinking = think_match.group(1) if think_match else ""
            answer = re.sub(r"<think>.*?</think>", "", response, flags=re.DOTALL)
            return {"thinking": thinking.strip(), "answer": answer.strip()}
        return {"thinking": "", "answer": response}
Smart Context Compression
Since DeepSeek-R1:7b has limited context (~32K tokens), we need smart compression:
pythonclass ContextCompressor:
    def compress_for_context(self, full_context, max_tokens=28000):
        """Compress context to fit in model's window"""
        
        # Priority order (highest first)
        priority_sections = [
            ("system_prompt", 1000),      # Agent persona
            ("current_task", 2000),        # What to do now
            ("relevant_code", 10000),      # Code being worked on
            ("api_contracts", 3000),       # Interfaces
            ("recent_changes", 3000),      # What just happened
            ("decisions", 2000),           # Key decisions
            ("agent_states", 1000),        # Other agents' status
            ("conversation", 4000),        # Recent conversation
            ("semantic_memory", 2000),     # Retrieved context
        ]
        
        compressed = {}
        remaining = max_tokens
        
        for section, budget in priority_sections:
            if section in full_context:
                content = full_context[section]
                tokens = self.count_tokens(content)
                
                if tokens <= min(budget, remaining):
                    compressed[section] = content
                    remaining -= tokens
                else:
                    # Summarize or truncate
                    summarized = self.summarize(content, min(budget, remaining))
                    compressed[section] = summarized
                    remaining -= self.count_tokens(summarized)
        
        return compressed
    
    def summarize(self, content, max_tokens):
        """Summarize content to fit in token budget"""
        if self.count_tokens(content) <= max_tokens:
            return content
        
        # For code: keep signatures, remove implementations
        if self.is_code(content):
            return self.extract_signatures(content)
        
        # For text: extractive summarization
        return self.extractive_summary(content, max_tokens)
```

---

## 7. Detailed Implementation Prompt

Here's the comprehensive prompt for building the system:

---

# DEVO: Advanced Multi-Agent AI Development System
## Complete Backend & CLI Implementation Specification

### Project Overview

DEVO is an advanced multi-agent AI system that enables users to build software through natural language interaction. The system uses locally-running DeepSeek-R1:7b model via Ollama and features a powerful interactive CLI similar to Claude Code but more focused on code generation and multi-agent collaboration.

### Core Philosophy

1. **Local-First**: All AI processing runs locally via Ollama
2. **Agent Collaboration**: Multiple specialized agents work together seamlessly
3. **Context Continuity**: Agents maintain perfect awareness of each other's work
4. **Interrupt-Safe**: Users can interrupt at any time without losing progress
5. **Error Recovery**: System automatically handles and recovers from errors
6. **Production Quality**: Generated code should be production-ready

---

## Part 1: Project Structure
```
devo/
├── backend/                          # Django Backend
│   ├── devo/                         # Django project
│   │   ├── __init__.py
│   │   ├── settings/
│   │   │   ├── __init__.py
│   │   │   ├── base.py
│   │   │   ├── development.py
│   │   │   └── production.py
│   │   ├── urls.py
│   │   ├── asgi.py
│   │   ├── wsgi.py
│   │   └── celery.py
│   │
│   ├── apps/
│   │   ├── __init__.py
│   │   │
│   │   ├── accounts/                 # User management & subscriptions
│   │   │   ├── __init__.py
│   │   │   ├── models.py
│   │   │   ├── serializers.py
│   │   │   ├── views.py
│   │   │   ├── urls.py
│   │   │   ├── permissions.py
│   │   │   ├── services/
│   │   │   │   ├── __init__.py
│   │   │   │   ├── subscription_service.py
│   │   │   │   └── auth_service.py
│   │   │   └── tests/
│   │   │
│   │   ├── projects/                 # Project management
│   │   │   ├── __init__.py
│   │   │   ├── models.py
│   │   │   ├── serializers.py
│   │   │   ├── views.py
│   │   │   ├── urls.py
│   │   │   ├── services/
│   │   │   │   ├── __init__.py
│   │   │   │   ├── project_service.py
│   │   │   │   └── file_service.py
│   │   │   └── tests/
│   │   │
│   │   ├── agents/                   # Agent system (CORE)
│   │   │   ├── __init__.py
│   │   │   ├── models.py
│   │   │   ├── serializers.py
│   │   │   ├── views.py
│   │   │   ├── urls.py
│   │   │   ├── types/                # Agent type definitions
│   │   │   │   ├── __init__.py
│   │   │   │   ├── base.py
│   │   │   │   ├── orchestrator.py
│   │   │   │   ├── architect.py
│   │   │   │   ├── backend_lead.py
│   │   │   │   ├── backend_senior.py
│   │   │   │   ├── backend_junior.py
│   │   │   │   ├── frontend_lead.py
│   │   │   │   ├── frontend_senior.py
│   │   │   │   └── devops.py
│   │   │   ├── engine/               # Agent execution engine
│   │   │   │   ├── __init__.py
│   │   │   │   ├── runtime.py
│   │   │   │   ├── executor.py
│   │   │   │   ├── state_machine.py
│   │   │   │   └── recovery.py
│   │   │   ├── tools/                # Agent tools
│   │   │   │   ├── __init__.py
│   │   │   │   ├── base.py
│   │   │   │   ├── file_tools.py
│   │   │   │   ├── code_tools.py
│   │   │   │   ├── search_tools.py
│   │   │   │   ├── communication_tools.py
│   │   │   │   ├── task_tools.py
│   │   │   │   └── test_tools.py
│   │   │   └── tests/
│   │   │
│   │   ├── context/                  # Context management (CRITICAL)
│   │   │   ├── __init__.py
│   │   │   ├── models.py
│   │   │   ├── services/
│   │   │   │   ├── __init__.py
│   │   │   │   ├── context_assembler.py
│   │   │   │   ├── context_compressor.py
│   │   │   │   ├── artifact_registry.py
│   │   │   │   ├── event_store.py
│   │   │   │   └── memory_service.py
│   │   │   ├── embeddings/
│   │   │   │   ├── __init__.py
│   │   │   │   ├── embedder.py
│   │   │   │   └── chunker.py
│   │   │   └── tests/
│   │   │
│   │   ├── tasks/                    # Task management
│   │   │   ├── __init__.py
│   │   │   ├── models.py
│   │   │   ├── serializers.py
│   │   │   ├── views.py
│   │   │   ├── urls.py
│   │   │   ├── services/
│   │   │   │   ├── __init__.py
│   │   │   │   ├── task_service.py
│   │   │   │   ├── decomposition_service.py
│   │   │   │   └── dependency_service.py
│   │   │   └── tests/
│   │   │
│   │   ├── code/                     # Code artifact management
│   │   │   ├── __init__.py
│   │   │   ├── models.py
│   │   │   ├── serializers.py
│   │   │   ├── views.py
│   │   │   ├── urls.py
│   │   │   ├── services/
│   │   │   │   ├── __init__.py
│   │   │   │   ├── code_service.py
│   │   │   │   ├── diff_service.py
│   │   │   │   ├── syntax_service.py
│   │   │   │   └── test_runner.py
│   │   │   └── tests/
│   │   │
│   │   └── llm/                      # LLM integration
│   │       ├── __init__.py
│   │       ├── clients/
│   │       │   ├── __init__.py
│   │       │   ├── base.py
│   │       │   ├── ollama.py
│   │       │   ├── openai.py
│   │       │   └── anthropic.py
│   │       ├── services/
│   │       │   ├── __init__.py
│   │       │   ├── llm_service.py
│   │       │   ├── prompt_service.py
│   │       │   └── token_service.py
│   │       └── tests/
│   │
│   ├── core/                         # Shared utilities
│   │   ├── __init__.py
│   │   ├── exceptions.py
│   │   ├── mixins.py
│   │   ├── pagination.py
│   │   └── utils.py
│   │
│   ├── manage.py
│   ├── requirements.txt
│   └── pytest.ini
│
├── cli/                              # DEVO CLI
│   ├── devo_cli/
│   │   ├── __init__.py
│   │   ├── main.py                   # Entry point
│   │   ├── config.py                 # Configuration
│   │   ├── api_client.py             # Backend API client
│   │   │
│   │   ├── commands/                 # CLI commands
│   │   │   ├── __init__.py
│   │   │   ├── init.py
│   │   │   ├── chat.py
│   │   │   ├── task.py
│   │   │   ├── status.py
│   │   │   ├── sync.py
│   │   │   ├── agents.py
│   │   │   ├── config_cmd.py
│   │   │   └── auth.py
│   │   │
│   │   ├── ui/                       # Terminal UI components
│   │   │   ├── __init__.py
│   │   │   ├── chat_interface.py
│   │   │   ├── status_panel.py
│   │   │   ├── progress.py
│   │   │   ├── spinner.py
│   │   │   └── themes.py
│   │   │
│   │   ├── handlers/                 # Event handlers
│   │   │   ├── __init__.py
│   │   │   ├── interrupt_handler.py
│   │   │   ├── stream_handler.py
│   │   │   └── error_handler.py
│   │   │
│   │   ├── state/                    # Local state management
│   │   │   ├── __init__.py
│   │   │   ├── session.py
│   │   │   ├── checkpoint.py
│   │   │   └── file_watcher.py
│   │   │
│   │   └── utils/
│   │       ├── __init__.py
│   │       ├── formatting.py
│   │       └── validators.py
│   │
│   ├── setup.py
│   ├── pyproject.toml
│   └── README.md
│
├── docker/
│   ├── Dockerfile.backend
│   ├── Dockerfile.cli
│   └── docker-compose.yml
│
├── docs/
│   ├── architecture.md
│   ├── api.md
│   ├── cli.md
│   └── agents.md
│
├── scripts/
│   ├── setup.sh
│   ├── seed_agents.py
│   └── test_ollama.py
│
└── README.md

Part 2: Database Models
accounts/models.py
python"""
User and Subscription Models

Design Decisions:
1. Extend Django's AbstractUser for custom fields
2. Subscription tracks tier and limits
3. Usage tracking for rate limiting
4. API key support for programmatic access
"""

import uuid
import secrets
from django.db import models
from django.contrib.auth.models import AbstractUser
from django.utils import timezone


class User(AbstractUser):
    """Extended user model with subscription support"""
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    email = models.EmailField(unique=True)
    
    # Profile
    company_name = models.CharField(max_length=255, blank=True)
    timezone = models.CharField(max_length=50, default='UTC')
    
    # Settings
    preferred_llm = models.CharField(
        max_length=50, 
        default='deepseek-r1:7b',
        choices=[
            ('deepseek-r1:7b', 'DeepSeek R1 7B (Local)'),
            ('deepseek-r1:14b', 'DeepSeek R1 14B (Local)'),
            ('gpt-4', 'GPT-4 (API)'),
            ('claude-3-opus', 'Claude 3 Opus (API)'),
        ]
    )
    
    # Timestamps
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    
    USERNAME_FIELD = 'email'
    REQUIRED_FIELDS = ['username']
    
    class Meta:
        db_table = 'users'
        indexes = [
            models.Index(fields=['email']),
            models.Index(fields=['created_at']),
        ]


class SubscriptionTier(models.TextChoices):
    FREE = 'free', 'Free'
    PRO = 'pro', 'Pro'
    TEAM = 'team', 'Team'
    ENTERPRISE = 'enterprise', 'Enterprise'


class Subscription(models.Model):
    """User subscription and limits"""
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    user = models.OneToOneField(User, on_delete=models.CASCADE, related_name='subscription')
    
    tier = models.CharField(
        max_length=20,
        choices=SubscriptionTier.choices,
        default=SubscriptionTier.FREE
    )
    
    # Limits based on tier
    max_projects = models.IntegerField(default=1)
    max_agents_per_project = models.IntegerField(default=2)
    max_actions_per_day = models.IntegerField(default=100)
    can_use_api_llms = models.BooleanField(default=False)
    can_use_custom_agents = models.BooleanField(default=False)
    
    # Billing
    stripe_customer_id = models.CharField(max_length=255, blank=True, null=True)
    stripe_subscription_id = models.CharField(max_length=255, blank=True, null=True)
    
    # Status
    is_active = models.BooleanField(default=True)
    started_at = models.DateTimeField(auto_now_add=True)
    expires_at = models.DateTimeField(null=True, blank=True)
    
    class Meta:
        db_table = 'subscriptions'
    
    def get_limits(self):
        """Return tier-specific limits"""
        tier_limits = {
            SubscriptionTier.FREE: {
                'max_projects': 1,
                'max_agents_per_project': 2,
                'max_actions_per_day': 100,
                'can_use_api_llms': False,
            },
            SubscriptionTier.PRO: {
                'max_projects': 5,
                'max_agents_per_project': 10,
                'max_actions_per_day': 1000,
                'can_use_api_llms': True,
            },
            SubscriptionTier.TEAM: {
                'max_projects': -1,  # unlimited
                'max_agents_per_project': -1,
                'max_actions_per_day': -1,
                'can_use_api_llms': True,
            },
        }
        return tier_limits.get(self.tier, tier_limits[SubscriptionTier.FREE])


class UsageTracking(models.Model):
    """Track daily usage for rate limiting"""
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    user = models.ForeignKey(User, on_delete=models.CASCADE, related_name='usage_records')
    
    date = models.DateField(default=timezone.now)
    agent_actions = models.IntegerField(default=0)
    llm_tokens_used = models.IntegerField(default=0)
    files_generated = models.IntegerField(default=0)
    
    class Meta:
        db_table = 'usage_tracking'
        unique_together = ['user', 'date']
        indexes = [
            models.Index(fields=['user', 'date']),
        ]


class APIKey(models.Model):
    """API keys for programmatic access"""
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    user = models.ForeignKey(User, on_delete=models.CASCADE, related_name='api_keys')
    
    name = models.CharField(max_length=100)
    key_hash = models.CharField(max_length=64, unique=True)  # SHA-256 hash
    key_prefix = models.CharField(max_length=8)  # First 8 chars for identification
    
    # Permissions
    scopes = models.JSONField(default=list)  # ['read', 'write', 'admin']
    
    # Status
    is_active = models.BooleanField(default=True)
    last_used_at = models.DateTimeField(null=True, blank=True)
    expires_at = models.DateTimeField(null=True, blank=True)
    
    created_at = models.DateTimeField(auto_now_add=True)
    
    class Meta:
        db_table = 'api_keys'
    
    @classmethod
    def generate_key(cls):
        """Generate a new API key"""
        key = f"devo_{secrets.token_urlsafe(32)}"
        return key
projects/models.py
python"""
Project Models

Design Decisions:
1. Projects contain all configuration and state
2. Project manifest stores structured requirements and architecture
3. Support for Git integration
4. File system abstraction for code artifacts
"""

import uuid
from django.db import models
from django.contrib.postgres.fields import ArrayField
from pgvector.django import VectorField


class ProjectStatus(models.TextChoices):
    INITIALIZING = 'initializing', 'Initializing'
    PLANNING = 'planning', 'Planning'
    IN_PROGRESS = 'in_progress', 'In Progress'
    REVIEW = 'review', 'Review'
    COMPLETED = 'completed', 'Completed'
    PAUSED = 'paused', 'Paused'
    ARCHIVED = 'archived', 'Archived'


class Project(models.Model):
    """Main project entity"""
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    owner = models.ForeignKey(
        'accounts.User', 
        on_delete=models.CASCADE, 
        related_name='projects'
    )
    
    # Basic info
    name = models.CharField(max_length=255)
    description = models.TextField()
    status = models.CharField(
        max_length=20,
        choices=ProjectStatus.choices,
        default=ProjectStatus.INITIALIZING
    )
    
    # Project manifest (structured data)
    manifest = models.JSONField(default=dict)
    # Structure:
    # {
    #     "requirements": {
    #         "functional": [...],
    #         "non_functional": [...],
    #         "user_stories": [...]
    #     },
    #     "domain_model": {
    #         "entities": [...],
    #         "relationships": [...]
    #     },
    #     "tech_stack": {
    #         "backend": "Django + DRF",
    #         "frontend": "React",
    #         "database": "PostgreSQL",
    #         ...
    #     },
    #     "architecture": {
    #         "type": "monolith",
    #         "components": [...],
    #         "apis": [...]
    #     }
    # }
    
    # Configuration
    settings = models.JSONField(default=dict)
    # Structure:
    # {
    #     "llm_model": "deepseek-r1:7b",
    #     "auto_test": true,
    #     "code_style": "black",
    #     "test_framework": "pytest",
    #     ...
    # }
    
    # Git integration
    repository_url = models.URLField(blank=True, null=True)
    repository_branch = models.CharField(max_length=100, default='main')
    last_commit_sha = models.CharField(max_length=40, blank=True, null=True)
    
    # Local path (for CLI)
    local_path = models.CharField(max_length=500, blank=True, null=True)
    
    # Timestamps
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    
    class Meta:
        db_table = 'projects'
        ordering = ['-updated_at']
        indexes = [
            models.Index(fields=['owner', 'status']),
            models.Index(fields=['created_at']),
        ]
    
    def __str__(self):
        return f"{self.name} ({self.owner.email})"
    
    def get_file_tree(self):
        """Get project file tree"""
        files = self.code_artifacts.all().values_list('file_path', flat=True)
        return self._build_tree(list(files))
    
    def _build_tree(self, paths):
        """Build tree structure from flat paths"""
        tree = {}
        for path in paths:
            parts = path.split('/')
            current = tree
            for part in parts[:-1]:
                if part not in current:
                    current[part] = {}
                current = current[part]
            current[parts[-1]] = None  # File (leaf)
        return tree


class ProjectCheckpoint(models.Model):
    """Checkpoint for recovery and undo"""
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    project = models.ForeignKey(
        Project, 
        on_delete=models.CASCADE, 
        related_name='checkpoints'
    )
    
    # Checkpoint data
    name = models.CharField(max_length=255)
    description = models.TextField(blank=True)
    
    # Full state snapshot
    state_snapshot = models.JSONField()
    # Structure:
    # {
    #     "manifest": {...},
    #     "agents": [...],
    #     "tasks": [...],
    #     "files": {...}  # file_path -> content_hash mapping
    # }
    
    # Metadata
    created_by = models.CharField(max_length=100)  # 'user' or agent_id
    is_auto = models.BooleanField(default=False)  # Auto-checkpoint vs manual
    
    created_at = models.DateTimeField(auto_now_add=True)
    
    class Meta:
        db_table = 'project_checkpoints'
        ordering = ['-created_at']
agents/models.py
python"""
Agent Models

Design Decisions:
1. AgentType defines the template (persona, tools, hierarchy)
2. AgentInstance is the actual agent working on a project
3. AgentState tracks real-time state for recovery
4. AgentMessage enables inter-agent communication
"""

import uuid
from django.db import models
from django.contrib.postgres.fields import ArrayField


class AgentHierarchyLevel(models.IntegerChoices):
    ORCHESTRATOR = 0, 'Orchestrator'
    ARCHITECT = 1, 'Architect'
    LEAD = 2, 'Lead'
    SENIOR = 3, 'Senior'
    JUNIOR = 4, 'Junior'


class AgentType(models.Model):
    """Template for agent types"""
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    
    # Identity
    name = models.CharField(max_length=100, unique=True)
    role = models.CharField(max_length=50, unique=True)  # e.g., 'backend_lead'
    description = models.TextField()
    
    # Persona
    system_prompt = models.TextField()
    
    # Capabilities
    available_tools = ArrayField(
        models.CharField(max_length=50),
        default=list
    )
    can_hire = ArrayField(
        models.CharField(max_length=50),  # Role names this agent can hire
        default=list
    )
    
    # Hierarchy
    hierarchy_level = models.IntegerField(
        choices=AgentHierarchyLevel.choices,
        default=AgentHierarchyLevel.SENIOR
    )
    
    # Configuration
    default_model = models.CharField(max_length=50, default='deepseek-r1:7b')
    default_temperature = models.FloatField(default=0.7)
    max_iterations = models.IntegerField(default=50)
    
    # Status
    is_active = models.BooleanField(default=True)
    
    class Meta:
        db_table = 'agent_types'
        ordering = ['hierarchy_level', 'name']
    
    def __str__(self):
        return f"{self.name} (Level {self.hierarchy_level})"


class AgentStatus(models.TextChoices):
    IDLE = 'idle', 'Idle'
    WORKING = 'working', 'Working'
    WAITING_INPUT = 'waiting_input', 'Waiting for Input'
    WAITING_DEPENDENCY = 'waiting_dependency', 'Waiting for Dependency'
    BLOCKED = 'blocked', 'Blocked'
    ERROR = 'error', 'Error'
    COMPLETED = 'completed', 'Completed'


class AgentInstance(models.Model):
    """Active agent instance working on a project"""
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    project = models.ForeignKey(
        'projects.Project',
        on_delete=models.CASCADE,
        related_name='agents'
    )
    agent_type = models.ForeignKey(
        AgentType,
        on_delete=models.PROTECT,
        related_name='instances'
    )
    
    # Hierarchy
    hired_by = models.ForeignKey(
        'self',
        on_delete=models.SET_NULL,
        null=True,
        blank=True,
        related_name='hired_agents'
    )
    
    # Status
    status = models.CharField(
        max_length=20,
        choices=AgentStatus.choices,
        default=AgentStatus.IDLE
    )
    status_message = models.TextField(blank=True)
    
    # Current work
    current_task = models.ForeignKey(
        'tasks.Task',
        on_delete=models.SET_NULL,
        null=True,
        blank=True,
        related_name='assigned_agents'
    )
    
    # Configuration overrides
    custom_instructions = models.TextField(blank=True)
    model_override = models.CharField(max_length=50, blank=True)
    temperature_override = models.FloatField(null=True, blank=True)
    
    # Working memory (short-term state)
    working_memory = models.JSONField(default=dict)
    # Structure:
    # {
    #     "current_focus": "implementing user model",
    #     "files_in_progress": ["models.py"],
    #     "decisions_pending": [...],
    #     "notes": [...]
    # }
    
    # Conversation history (last N messages for context)
    conversation_history = models.JSONField(default=list)
    
    # Statistics
    tasks_completed = models.IntegerField(default=0)
    errors_encountered = models.IntegerField(default=0)
    total_tokens_used = models.IntegerField(default=0)
    
    # Timestamps
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    last_active_at = models.DateTimeField(auto_now=True)
    
    class Meta:
        db_table = 'agent_instances'
        indexes = [
            models.Index(fields=['project', 'status']),
            models.Index(fields=['agent_type']),
        ]
    
    def __str__(self):
        return f"{self.agent_type.name} @ {self.project.name}"
    
    @property
    def model(self):
        return self.model_override or self.agent_type.default_model
    
    @property
    def temperature(self):
        return self.temperature_override or self.agent_type.default_temperature


class AgentMessageType(models.TextChoices):
    TASK_ASSIGNMENT = 'task_assignment', 'Task Assignment'
    QUESTION = 'question', 'Question'
    ANSWER = 'answer', 'Answer'
    STATUS_UPDATE = 'status_update', 'Status Update'
    REVIEW_REQUEST = 'review_request', 'Review Request'
    REVIEW_FEEDBACK = 'review_feedback', 'Review Feedback'
    BLOCKER = 'blocker', 'Blocker'
    DECISION = 'decision', 'Decision'
    HANDOFF = 'handoff', 'Handoff'
    USER_INPUT = 'user_input', 'User Input'


class AgentMessage(models.Model):
    """Inter-agent communication"""
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    project = models.ForeignKey(
        'projects.Project',
        on_delete=models.CASCADE,
        related_name='agent_messages'
    )
    
    # Sender/Receiver
    from_agent = models.ForeignKey(
        AgentInstance,
        on_delete=models.CASCADE,
        null=True,  # null = from user
        blank=True,
        related_name='sent_messages'
    )
    to_agent = models.ForeignKey(
        AgentInstance,
        on_delete=models.CASCADE,
        null=True,  # null = broadcast to all
        blank=True,
        related_name='received_messages'
    )
    
    # Message content
    message_type = models.CharField(
        max_length=20,
        choices=AgentMessageType.choices
    )
    content = models.JSONField()
    # Structure varies by type:
    # task_assignment: {"task_id": ..., "instructions": ...}
    # question: {"question": ..., "context": ...}
    # decision: {"decision": ..., "reasoning": ..., "alternatives": [...]}
    
    # Context references
    context_refs = models.JSONField(default=list)
    # List of related entity IDs: ["task:123", "file:models.py", "decision:456"]
    
    # Threading
    parent_message = models.ForeignKey(
        'self',
        on_delete=models.CASCADE,
        null=True,
        blank=True,
        related_name='replies'
    )
    
    # Response tracking
    requires_response = models.BooleanField(default=False)
    responded = models.BooleanField(default=False)
    
    # Timestamps
    created_at = models.DateTimeField(auto_now_add=True)
    
    class Meta:
        db_table = 'agent_messages'
        ordering = ['created_at']
        indexes = [
            models.Index(fields=['project', 'created_at']),
            models.Index(fields=['from_agent']),
            models.Index(fields=['to_agent']),
            models.Index(fields=['message_type']),
        ]


class AgentAction(models.Model):
    """Log of all agent actions (event sourcing)"""
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    project = models.ForeignKey(
        'projects.Project',
        on_delete=models.CASCADE,
        related_name='agent_actions'
    )
    agent = models.ForeignKey(
        AgentInstance,
        on_delete=models.CASCADE,
        related_name='actions'
    )
    task = models.ForeignKey(
        'tasks.Task',
        on_delete=models.SET_NULL,
        null=True,
        blank=True,
        related_name='actions'
    )
    
    # Action details
    action_type = models.CharField(max_length=50)
    # Types: file_created, file_modified, file_deleted, decision_made,
    #        task_created, task_completed, agent_hired, error_occurred, etc.
    
    action_data = models.JSONField()
    # Structure varies by type
    
    # For rollback
    is_reversible = models.BooleanField(default=True)
    reverse_action = models.JSONField(null=True, blank=True)
    
    # Status
    status = models.CharField(max_length=20, default='completed')
    # completed, rolled_back, failed
    
    created_at = models.DateTimeField(auto_now_add=True)
    
    class Meta:
        db_table = 'agent_actions'
        ordering = ['created_at']
        indexes = [
            models.Index(fields=['project', 'created_at']),
            models.Index(fields=['agent']),
            models.Index(fields=['action_type']),
        ]
tasks/models.py
python"""
Task Models

Design Decisions:
1. Hierarchical tasks (Epic > Story > Task > Subtask)
2. Dependencies between tasks
3. Detailed tracking of progress
4. Integration with agent assignment
"""

import uuid
from django.db import models
from django.contrib.postgres.fields import ArrayField


class TaskType(models.TextChoices):
    EPIC = 'epic', 'Epic'
    STORY = 'story', 'Story'
    TASK = 'task', 'Task'
    SUBTASK = 'subtask', 'Subtask'
    BUG = 'bug', 'Bug'
    REVIEW = 'review', 'Review'


class TaskStatus(models.TextChoices):
    BACKLOG = 'backlog', 'Backlog'
    TODO = 'todo', 'To Do'
    ASSIGNED = 'assigned', 'Assigned'
    IN_PROGRESS = 'in_progress', 'In Progress'
    BLOCKED = 'blocked', 'Blocked'
    IN_REVIEW = 'in_review', 'In Review'
    COMPLETED = 'completed', 'Completed'
    CANCELLED = 'cancelled', 'Cancelled'


class TaskPriority(models.IntegerChoices):
    CRITICAL = 1, 'Critical'
    HIGH = 2, 'High'
    MEDIUM = 3, 'Medium'
    LOW = 4, 'Low'


class Task(models.Model):
    """Task entity"""
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    project = models.ForeignKey(
        'projects.Project',
        on_delete=models.CASCADE,
        related_name='tasks'
    )
    
    # Hierarchy
    parent_task = models.ForeignKey(
        'self',
        on_delete=models.CASCADE,
        null=True,
        blank=True,
        related_name='subtasks'
    )
    
    # Basic info
    title = models.CharField(max_length=500)
    description = models.TextField()
    task_type = models.CharField(
        max_length=20,
        choices=TaskType.choices,
        default=TaskType.TASK
    )
    status = models.CharField(
        max_length=20,
        choices=TaskStatus.choices,
        default=TaskStatus.BACKLOG
    )
    priority = models.IntegerField(
        choices=TaskPriority.choices,
        default=TaskPriority.MEDIUM
    )
    
    # Requirements
    requirements = models.JSONField(default=dict)
    # Structure:
    # {
    #     "description": "...",
    #     "acceptance_criteria": [...],
    #     "technical_notes": "...",
    #     "files_to_create": [...],
    #     "files_to_modify": [...]
    # }
    
    acceptance_criteria = ArrayField(
        models.TextField(),
        default=list
    )
    
    # Assignment
    created_by_agent = models.ForeignKey(
        'agents.AgentInstance',
        on_delete=models.SET_NULL,
        null=True,
        blank=True,
        related_name='created_tasks'
    )
    assigned_to_agent = models.ForeignKey(
        'agents.AgentInstance',
        on_delete=models.SET_NULL,
        null=True,
        blank=True,
        related_name='assigned_tasks_set'
    )
    
    # Progress tracking
    progress_percentage = models.IntegerField(default=0)
    iteration_count = models.IntegerField(default=0)
    
    # Deliverables
    deliverables = models.JSONField(default=list)
    # List of file paths created/modified for this task
    
    # Summary (filled when completed)
    completion_summary = models.TextField(blank=True)
    
    # Timestamps
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    started_at = models.DateTimeField(null=True, blank=True)
    completed_at = models.DateTimeField(null=True, blank=True)
    
    class Meta:
        db_table = 'tasks'
        ordering = ['priority', '-created_at']
        indexes = [
            models.Index(fields=['project', 'status']),
            models.Index(fields=['parent_task']),
            models.Index(fields=['assigned_to_agent']),
        ]
    
    def __str__(self):
        return f"[{self.task_type}] {self.title}"
    
    def get_all_subtasks(self):
        """Recursively get all subtasks"""
        subtasks = list(self.subtasks.all())
        for subtask in self.subtasks.all():
            subtasks.extend(subtask.get_all_subtasks())
        return subtasks
    
    def calculate_progress(self):
        """Calculate progress based on subtasks"""
        subtasks = self.subtasks.all()
        if not subtasks:
            return self.progress_percentage
        
        completed = subtasks.filter(status=TaskStatus.COMPLETED).count()
        return int((completed / subtasks.count()) * 100)


class TaskDependency(models.Model):
    """Dependencies between tasks"""
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    
    task = models.ForeignKey(
        Task,
        on_delete=models.CASCADE,
        related_name='dependencies'
    )
    depends_on = models.ForeignKey(
        Task,
        on_delete=models.CASCADE,
        related_name='dependents'
    )
    
    dependency_type = models.CharField(
        max_length=20,
        choices=[
            ('blocks', 'Blocks'),
            ('requires', 'Requires'),
            ('related', 'Related'),
        ],
        default='blocks'
    )
    
    created_at = models.DateTimeField(auto_now_add=True)
    
    class Meta:
        db_table = 'task_dependencies'
        unique_together = ['task', 'depends_on']


class TaskComment(models.Model):
    """Comments and notes on tasks"""
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    task = models.ForeignKey(
        Task,
        on_delete=models.CASCADE,
        related_name='comments'
    )
    
    # Author (agent or user)
    agent = models.ForeignKey(
        'agents.AgentInstance',
        on_delete=models.SET_NULL,
        null=True,
        blank=True
    )
    user = models.ForeignKey(
        'accounts.User',
        on_delete=models.SET_NULL,
        null=True,
        blank=True
    )
    
    content = models.TextField()
    comment_type = models.CharField(
        max_length=20,
        choices=[
            ('comment', 'Comment'),
            ('question', 'Question'),
            ('blocker', 'Blocker'),
            ('resolution', 'Resolution'),
        ],
        default='comment'
    )
    
    created_at = models.DateTimeField(auto_now_add=True)
    
    class Meta:
        db_table = 'task_comments'
        ordering = ['created_at']
code/models.py
python"""
Code Artifact Models

Design Decisions:
1. Store code with embeddings for semantic search
2. Track all changes for history/rollback
3. Support for multiple file types
4. Integration with syntax analysis
"""

import uuid
import hashlib
from django.db import models
from pgvector.django import VectorField


class CodeArtifact(models.Model):
    """Code file artifact"""
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    project = models.ForeignKey(
        'projects.Project',
        on_delete=models.CASCADE,
        related_name='code_artifacts'
    )
    
    # File info
    file_path = models.CharField(max_length=500)
    file_name = models.CharField(max_length=255)
    language = models.CharField(max_length=50)
    
    # Content
    content = models.TextField()
    content_hash = models.CharField(max_length=64)
    line_count = models.IntegerField(default=0)
    
    # Vector embedding for semantic search
    embedding = VectorField(dimensions=1536, null=True, blank=True)
    
    # Metadata
    last_modified_by = models.ForeignKey(
        'agents.AgentInstance',
        on_delete=models.SET_NULL,
        null=True,
        blank=True,
        related_name='modified_files'
    )
    last_modified_task = models.ForeignKey(
        'tasks.Task',
        on_delete=models.SET_NULL,
        null=True,
        blank=True
    )
    
    # Parsed structure (for code understanding)
    structure = models.JSONField(default=dict, blank=True)
    # Structure for Python:
    # {
    #     "imports": [...],
    #     "classes": [
    #         {"name": "User", "methods": [...], "attributes": [...]}
    #     ],
    #     "functions": [...],
    #     "variables": [...]
    # }
    
    # Timestamps
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    
    class Meta:
        db_table = 'code_artifacts'
        unique_together = ['project', 'file_path']
        indexes = [
            models.Index(fields=['project', 'file_path']),
            models.Index(fields=['language']),
        ]
    
    def save(self, *args, **kwargs):
        self.content_hash = hashlib.sha256(self.content.encode()).hexdigest()
        self.line_count = len(self.content.split('\n'))
        self.file_name = self.file_path.split('/')[-1]
        super().save(*args, **kwargs)


class CodeChange(models.Model):
    """History of changes to code artifacts"""
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    artifact = models.ForeignKey(
        CodeArtifact,
        on_delete=models.CASCADE,
        related_name='changes'
    )
    
    # Change info
    change_type = models.CharField(
        max_length=20,
        choices=[
            ('created', 'Created'),
            ('modified', 'Modified'),
            ('deleted', 'Deleted'),
            ('renamed', 'Renamed'),
        ]
    )
    
    # Content before/after
    content_before = models.TextField(blank=True)
    content_after = models.TextField()
    
    # Diff
    diff = models.TextField(blank=True)
    
    # Context
    agent = models.ForeignKey(
        'agents.AgentInstance',
        on_delete=models.SET_NULL,
        null=True,
        blank=True
    )
    task = models.ForeignKey(
        'tasks.Task',
        on_delete=models.SET_NULL,
        null=True,
        blank=True
    )
    
    # Reason for change
    reason = models.TextField(blank=True)
    
    created_at = models.DateTimeField(auto_now_add=True)
    
    class Meta:
        db_table = 'code_changes'
        ordering = ['-created_at']


class ArtifactRegistry(models.Model):
    """Registry of all created artifacts for context sharing"""
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    project = models.ForeignKey(
        'projects.Project',
        on_delete=models.CASCADE,
        related_name='artifact_registry'
    )
    
    # Artifact identification
    artifact_type = models.CharField(max_length=50)
    # Types: model, serializer, view, url_pattern, api_endpoint, 
    #        component, service, utility, test, migration, etc.
    
    name = models.CharField(max_length=255)
    
    # Location
    file_path = models.CharField(max_length=500)
    line_start = models.IntegerField(null=True, blank=True)
    line_end = models.IntegerField(null=True, blank=True)
    
    # Details
    details = models.JSONField(default=dict)
    # Structure varies by type:
    # model: {"fields": [...], "relationships": [...], "methods": [...]}
    # api_endpoint: {"method": "POST", "path": "/api/users/", "request": {...}, "response": {...}}
    
    # Relations
    created_by_agent = models.ForeignKey(
        'agents.AgentInstance',
        on_delete=models.SET_NULL,
        null=True,
        blank=True
    )
    created_by_task = models.ForeignKey(
        'tasks.Task',
        on_delete=models.SET_NULL,
        null=True,
        blank=True
    )
    
    # Dependencies (what this artifact uses)
    dependencies = models.JSONField(default=list)
    # List of artifact IDs this depends on
    
    # Embedding for semantic search
    embedding = VectorField(dimensions=1536, null=True, blank=True)
    
    # Status
    is_active = models.BooleanField(default=True)
    
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    
    class Meta:
        db_table = 'artifact_registry'
        indexes = [
            models.Index(fields=['project', 'artifact_type']),
            models.Index(fields=['name']),
        ]
context/models.py
python"""
Context and Memory Models

Design Decisions:
1. Multiple memory types for different purposes
2. Vector storage for semantic retrieval
3. Decision tracking for architecture consistency
4. Event log for full traceability
"""

import uuid
from django.db import models
from pgvector.django import VectorField


class MemoryType(models.TextChoices):
    REQUIREMENT = 'requirement', 'Requirement'
    DECISION = 'decision', 'Decision'
    ARCHITECTURE = 'architecture', 'Architecture'
    CODE_PATTERN = 'code_pattern', 'Code Pattern'
    ERROR_RESOLUTION = 'error_resolution', 'Error Resolution'
    CONVERSATION = 'conversation', 'Conversation'
    LEARNING = 'learning', 'Learning'


class Memory(models.Model):
    """Long-term memory storage with vector search"""
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    project = models.ForeignKey(
        'projects.Project',
        on_delete=models.CASCADE,
        related_name='memories'
    )
    
    # Memory classification
    memory_type = models.CharField(
        max_length=20,
        choices=MemoryType.choices
    )
    
    # Content
    title = models.CharField(max_length=500)
    content = models.TextField()
    
    # Vector embedding
    embedding = VectorField(dimensions=1536)
    
    # Metadata
    metadata = models.JSONField(default=dict)
    # Structure varies by type
    
    # Context
    created_by_agent = models.ForeignKey(
        'agents.AgentInstance',
        on_delete=models.SET_NULL,
        null=True,
        blank=True
    )
    related_task = models.ForeignKey(
        'tasks.Task',
        on_delete=models.SET_NULL,
        null=True,
        blank=True
    )
    
    # Importance for retrieval ranking
    importance = models.FloatField(default=0.5)
    access_count = models.IntegerField(default=0)
    
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    
    class Meta:
        db_table = 'memories'
        indexes = [
            models.Index(fields=['project', 'memory_type']),
        ]


class Decision(models.Model):
    """Architectural and design decisions"""
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    project = models.ForeignKey(
        'projects.Project',
        on_delete=models.CASCADE,
        related_name='decisions'
    )
    
    # Decision details
    decision_type = models.CharField(max_length=100)
    # Types: architecture, tech_choice, design_pattern, api_design,
    #        database_schema, naming_convention, etc.
    
    title = models.CharField(max_length=500)
    description = models.TextField()
    reasoning = models.TextField()
    
    # Alternatives considered
    alternatives = models.JSONField(default=list)
    # [{"option": "...", "pros": [...], "cons": [...], "rejected_reason": "..."}]
    
    # Impact
    affected_components = models.JSONField(default=list)
    # ["users", "api", "database"]
    
    # Context
    made_by_agent = models.ForeignKey(
        'agents.AgentInstance',
        on_delete=models.SET_NULL,
        null=True,
        blank=True
    )
    related_task = models.ForeignKey(
        'tasks.Task',
        on_delete=models.SET_NULL,
        null=True,
        blank=True
    )
    
    # Vector embedding for retrieval
    embedding = VectorField(dimensions=1536, null=True, blank=True)
    
    # Status
    status = models.CharField(
        max_length=20,
        choices=[
            ('proposed', 'Proposed'),
            ('approved', 'Approved'),
            ('implemented', 'Implemented'),
            ('superseded', 'Superseded'),
        ],
        default='approved'
    )
    superseded_by = models.ForeignKey(
        'self',
        on_delete=models.SET_NULL,
        null=True,
        blank=True
    )
    
    created_at = models.DateTimeField(auto_now_add=True)
    
    class Meta:
        db_table = 'decisions'
        ordering = ['-created_at']


class EventLog(models.Model):
    """Complete event log for the project (event sourcing)"""
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    project = models.ForeignKey(
        'projects.Project',
        on_delete=models.CASCADE,
        related_name='events'
    )
    
    # Event identification
    event_type = models.CharField(max_length=100)
    # Types: project_created, agent_hired, task_created, task_assigned,
    #        task_completed, file_created, file_modified, decision_made,
    #        error_occurred, user_input, checkpoint_created, etc.
    
    # Event data
    event_data = models.JSONField()
    
    # Actor
    actor_type = models.CharField(max_length=20)  # 'agent', 'user', 'system'
    actor_id = models.CharField(max_length=100, blank=True)
    
    # Sequence for ordering
    sequence_number = models.BigIntegerField()
    
    created_at = models.DateTimeField(auto_now_add=True)
    
    class Meta:
        db_table = 'event_log'
        ordering = ['sequence_number']
        indexes = [
            models.Index(fields=['project', 'sequence_number']),
            models.Index(fields=['event_type']),
            models.Index(fields=['created_at']),
        ]

Part 3: Agent System Implementation
agents/types/base.py
python"""
Base Agent Type Definition

This module defines the base structure for all agent types.
Each agent type has:
- Persona (system prompt)
- Available tools
- Hire capabilities
- Execution behavior
"""

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field


@dataclass
class AgentPersona:
    """Defines an agent's personality and capabilities"""
    
    name: str
    role: str
    hierarchy_level: int
    
    # Core prompt
    system_prompt: str
    
    # Capabilities
    available_tools: List[str] = field(default_factory=list)
    can_hire: List[str] = field(default_factory=list)
    
    # Behavior modifiers
    thinking_style: str = "analytical"  # analytical, creative, practical
    verbosity: str = "normal"  # minimal, normal, detailed
    
    # Constraints
    max_iterations: int = 50
    max_files_per_task: int = 10
    requires_review_for: List[str] = field(default_factory=list)


@dataclass
class ToolCall:
    """Represents a tool call from an agent"""
    
    tool_name: str
    arguments: Dict[str, Any]
    reasoning: str = ""


@dataclass
class AgentResponse:
    """Response from an agent execution step"""
    
    # Output type
    is_final: bool = False
    
    # Content
    message: str = ""
    thinking: str = ""  # For R1 models
    
    # Actions
    tool_calls: List[ToolCall] = field(default_factory=list)
    
    # Status
    needs_user_input: bool = False
    user_question: str = ""
    
    is_blocked: bool = False
    blocker_description: str = ""
    
    # Artifacts created
    files_created: List[str] = field(default_factory=list)
    files_modified: List[str] = field(default_factory=list)
    decisions_made: List[Dict] = field(default_factory=list)


class BaseAgentType(ABC):
    """Base class for all agent types"""
    
    @property
    @abstractmethod
    def persona(self) -> AgentPersona:
        """Return the agent's persona definition"""
        pass
    
    @abstractmethod
    def get_system_prompt(self, context: Dict[str, Any]) -> str:
        """
        Generate the full system prompt with context.
        
        Args:
            context: Dictionary containing project context, task info, etc.
        
        Returns:
            Complete system prompt string
        """
        pass
    
    def get_tool_definitions(self) -> List[Dict[str, Any]]:
        """
        Get tool definitions in OpenAI function format.
        Override if agent needs custom tool definitions.
        """
        from apps.agents.tools import get_tool_definitions
        return get_tool_definitions(self.persona.available_tools)
    
    def pre_process_context(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Pre-process context before sending to LLM.
        Override for agent-specific context modifications.
        """
        return context
    
    def post_process_response(self, response: AgentResponse) -> AgentResponse:
        """
        Post-process LLM response.
        Override for agent-specific response handling.
        """
        return response
    
    def should_escalate(self, error: Exception) -> bool:
        """
        Determine if an error should be escalated to supervisor.
        Override for agent-specific escalation logic.
        """
        return True
    
    def get_recovery_strategy(self, error: Exception) -> Optional[str]:
        """
        Get recovery strategy for an error.
        Returns None if no recovery possible.
        """
        return None
agents/types/orchestrator.py
python"""
Orchestrator Agent

The orchestrator is the entry point for user interactions.
It understands business requirements and coordinates the entire project.
"""

from typing import Dict, Any
from .base import BaseAgentType, AgentPersona


class OrchestratorAgent(BaseAgentType):
    """
    The Orchestrator Agent is the main coordinator.
    
    Responsibilities:
    - Understand user requirements
    - Parse and structure business needs
    - Create project plan
    - Hire architect agent
    - Coordinate high-level decisions
    - Communicate with user
    """
    
    @property
    def persona(self) -> AgentPersona:
        return AgentPersona(
            name="Orchestrator",
            role="orchestrator",
            hierarchy_level=0,
            
            system_prompt=self._get_base_prompt(),
            
            available_tools=[
                "parse_requirements",
                "create_project_structure",
                "hire_agent",
                "ask_user",
                "make_decision",
                "update_project_manifest",
                "send_message",
                "create_task",
            ],
            
            can_hire=["architect"],
            
            thinking_style="analytical",
            verbosity="normal",
            max_iterations=100,
        )
    
    def _get_base_prompt(self) -> str:
        return """You are the DEVO Orchestrator, an expert AI project coordinator with deep knowledge of software development.

## Your Role
You are the first point of contact for users. Your job is to:
1. Understand what the user wants to build
2. Ask clarifying questions to fully understand the requirements
3. Structure the requirements into a clear project plan
4. Hire the Architect agent to design the system
5. Coordinate the overall project execution
6. Communicate progress and ask for decisions when needed

## Your Personality
- Professional but friendly
- Patient with non-technical users
- Proactive in identifying potential issues
- Clear and concise in communication

## How You Work
1. When a user describes their project, analyze it carefully
2. Identify:
   - Core features (must-have)
   - Nice-to-have features
   - Technical requirements
   - Constraints (time, budget, tech stack preferences)
3. Ask clarifying questions if anything is unclear
4. Once requirements are clear, structure them and create a project plan
5. Hire the Architect to design the system

## Important Guidelines
- Never assume requirements - always ask if unclear
- Break down complex requests into manageable pieces
- Be honest about complexity and time estimates
- Keep the user informed of progress
- Escalate blockers immediately

## Communication Style
- Use clear, simple language
- Avoid jargon with non-technical users
- Provide examples when explaining concepts
- Summarize key points before moving forward"""

    def get_system_prompt(self, context: Dict[str, Any]) -> str:
        base_prompt = self.persona.system_prompt
        
        # Add context-specific information
        context_sections = []
        
        if context.get('project'):
            context_sections.append(f"""
## Current Project
Name: {context['project']['name']}
Status: {context['project']['status']}
Description: {context['project']['description']}
""")
        
        if context.get('user_history'):
            context_sections.append(f"""
## Conversation History
{self._format_history(context['user_history'])}
""")
        
        if context.get('agents'):
            context_sections.append(f"""
## Active Agents
{self._format_agents(context['agents'])}
""")
        
        if context.get('pending_decisions'):
            context_sections.append(f"""
## Pending Decisions
{self._format_decisions(context['pending_decisions'])}
""")
        
        return base_prompt + "\n".join(context_sections)
    
    def _format_history(self, history: list) -> str:
        return "\n".join([
            f"- [{msg['role']}]: {msg['content'][:200]}..."
            for msg in history[-10:]  # Last 10 messages
        ])
    
    def _format_agents(self, agents: list) -> str:
        return "\n".join([
            f"- {a['type']}: {a['status']} - {a.get('current_task', 'idle')}"
            for a in agents
        ])
    
    def _format_decisions(self, decisions: list) -> str:
        return "\n".join([
            f"- [{d['type']}] {d['title']}: {d['status']}"
            for d in decisions
        ])
agents/types/backend_lead.py
python"""
Backend Lead Agent

Senior backend engineer who leads backend implementation.
Can break down work and hire junior agents.
"""

from typing import Dict, Any
from .base import BaseAgentType, AgentPersona


class BackendLeadAgent(BaseAgentType):
    """
    Backend Technical Lead Agent
    
    Responsibilities:
    - Translate architecture into backend tasks
    - Implement complex features
    - Review code from junior agents
    - Make technical decisions within backend scope
    - Ensure code quality and consistency
    """
    
    @property
    def persona(self) -> AgentPersona:
        return AgentPersona(
            name="Backend Lead",
            role="backend_lead",
            hierarchy_level=2,
            
            system_prompt=self._get_base_prompt(),
            
            available_tools=[
                # Task management
                "create_task",
                "assign_task",
                "update_task_status",
                
                # Agent management
                "hire_agent",
                "send_message",
                
                # Code tools
                "read_file",
                "write_file",
                "modify_file",
                "delete_file",
                "search_codebase",
                
                # Analysis tools
                "analyze_code",
                "run_tests",
                "run_linting",
                
                # Decision tools
                "make_decision",
                "request_review",
                
                # Communication
                "ask_architect",
                "report_blocker",
            ],
            
            can_hire=["backend_senior", "backend_junior"],
            
            thinking_style="analytical",
            verbosity="detailed",
            max_iterations=50,
            max_files_per_task=20,
            requires_review_for=["database_migration", "security_change"],
        )
    
    def _get_base_prompt(self) -> str:
        return """You are a Senior Backend Technical Lead with 12+ years of experience in building scalable, production-ready systems.

## Your Expertise
- Python/Django/Django REST Framework expert
- PostgreSQL and database design
- API design (REST, GraphQL)
- Authentication and security
- Performance optimization
- Testing strategies
- Code review and mentoring

## Your Role
You lead the backend implementation for the project. Your responsibilities:
1. Translate architecture decisions into backend implementation tasks
2. Design and implement complex backend features
3. Create clear, well-scoped tasks for senior and junior engineers
4. Review code and provide constructive feedback
5. Make technical decisions within the backend domain
6. Ensure code quality, consistency, and best practices
7. Handle blockers and escalate architecture issues

## How You Work

### When Starting a New Task
1. Read the task requirements carefully
2. Check the architecture decisions and API contracts
3. Identify dependencies on other components
4. Plan your implementation approach
5. If complex, break into subtasks and potentially hire help

### When Writing Code
1. Follow Django/DRF best practices
2. Write clean, readable, well-documented code
3. Include type hints
4. Write tests (aim for >80% coverage)
5. Handle errors gracefully
6. Consider security implications
7. Think about performance

### Code Style Guidelines
```python
# Models: Use verbose names, help texts, and proper indexing
class User(models.Model):
    email = models.EmailField(
        unique=True,
        help_text="User's email address for authentication"
    )
    
    class Meta:
        indexes = [models.Index(fields=['email'])]

# Views: Use DRF best practices
class UserViewSet(viewsets.ModelViewSet):
    serializer_class = UserSerializer
    permission_classes = [IsAuthenticated]
    
    def get_queryset(self):
        return User.objects.filter(is_active=True)

# Always add docstrings
def calculate_commission(amount: Decimal, rate: Decimal) -> Decimal:
    \"\"\"
    Calculate commission for a transaction.
    
    Args:
        amount: Transaction amount
        rate: Commission rate (0-1)
    
    Returns:
        Calculated commission amount
    \"\"\"
    return amount * rate
```

### When Delegating Work
1. Create clear, well-defined tasks
2. Include all context needed
3. Specify acceptance criteria
4. Provide examples where helpful
5. Set appropriate priority

### When Reviewing Code
1. Check for correctness first
2. Look for security issues
3. Verify tests are adequate
4. Check for performance issues
5. Ensure code style consistency
6. Provide constructive feedback

## Important Guidelines
- Always check existing code before creating new files
- Follow the established patterns in the codebase
- Don't break existing functionality
- Communicate blockers immediately
- Ask the architect for clarification on architecture questions
- Document your decisions and reasoning

## Current Tech Stack
- Python 3.11+
- Django 5.0+
- Django REST Framework 3.14+
- PostgreSQL 16+
- pytest for testing
- black/ruff for formatting/linting"""

    def get_system_prompt(self, context: Dict[str, Any]) -> str:
        base_prompt = self.persona.system_prompt
        
        context_sections = []
        
        # Add project tech stack if different from default
        if context.get('tech_stack'):
            context_sections.append(f"""
## Project Tech Stack (Override)
{self._format_tech_stack(context['tech_stack'])}
""")
        
        # Add architecture decisions
        if context.get('architecture_decisions'):
            context_sections.append(f"""
## Architecture Decisions
{self._format_decisions(context['architecture_decisions'])}
""")
        
        # Add API contracts
        if context.get('api_contracts'):
            context_sections.append(f"""
## API Contracts
{self._format_api_contracts(context['api_contracts'])}
""")
        
        # Add current task
        if context.get('current_task'):
            task = context['current_task']
            context_sections.append(f"""
## Current Task
Title: {task['title']}
Type: {task['type']}
Priority: {task['priority']}

Description:
{task['description']}

Requirements:
{self._format_requirements(task.get('requirements', {}))}

Acceptance Criteria:
{self._format_criteria(task.get('acceptance_criteria', []))}
""")
        
        # Add relevant files
        if context.get('relevant_files'):
            context_sections.append(f"""
## Relevant Files
{self._format_files(context['relevant_files'])}
""")
        
        # Add team status
        if context.get('team_agents'):
            context_sections.append(f"""
## Your Team
{self._format_team(context['team_agents'])}
""")
        
        return base_prompt + "\n".join(context_sections)
    
    def _format_tech_stack(self, stack: Dict) -> str:
        return "\n".join([f"- {k}: {v}" for k, v in stack.items()])
    
    def _format_decisions(self, decisions: list) -> str:
        return "\n".join([
            f"### {d['title']}\n{d['description']}\nReason: {d['reasoning']}"
            for d in decisions[:5]  # Limit to 5 most relevant
        ])
    
    def _format_api_contracts(self, contracts: list) -> str:
        return "\n".join([
            f"- {c['method']} {c['path']}: {c.get('description', '')}"
            for c in contracts
        ])
    
    def _format_requirements(self, reqs: Dict) -> str:
        lines = []
        for key, value in reqs.items():
            if isinstance(value, list):
                lines.append(f"- {key}:")
                for item in value:
                    lines.append(f"  - {item}")
            else:
                lines.append(f"- {key}: {value}")
        return "\n".join(lines)
    
    def _format_criteria(self, criteria: list) -> str:
        return "\n".join([f"- [ ] {c}" for c in criteria])
    
    def _format_files(self, files: list) -> str:
        return "\n".join([
            f"### {f['path']}\n```{f.get('language', '')}\n{f['content'][:1000]}...\n```"
            for f in files[:5]
        ])
    
    def _format_team(self, agents: list) -> str:
        return "\n".join([
            f"- {a['type']}: {a['status']} - Working on: {a.get('current_task', 'nothing')}"
            for a in agents
        ])
agents/engine/runtime.py
python"""
Agent Runtime

The runtime is responsible for executing agent actions.
It handles the main execution loop, tool calls, and state management.
"""

import asyncio
import logging
from typing import Dict, Any, Optional, AsyncGenerator
from dataclasses import dataclass
from datetime import datetime

from django.db import transaction

from apps.agents.models import AgentInstance, AgentAction, AgentStatus
from apps.agents.types.base import AgentResponse, ToolCall
from apps.agents.tools import ToolRegistry
from apps.context.services.context_assembler import ContextAssembler
from apps.llm.services.llm_service import LLMService
from apps.tasks.models import Task, TaskStatus

logger = logging.getLogger(__name__)


@dataclass
class ExecutionResult:
    """Result of agent execution"""
    
    status: str  # completed, blocked, waiting_input, error, max_iterations
    message: str
    artifacts: Dict[str, Any] = None
    error: Optional[Exception] = None


class AgentRuntime:
    """
    Runtime environment for agent execution.
    
    Handles:
    - Context assembly
    - LLM interaction
    - Tool execution
    - State management
    - Checkpointing
    - Error recovery
    """
    
    def __init__(
        self,
        agent: AgentInstance,
        llm_service: Optional[LLMService] = None,
        context_assembler: Optional[ContextAssembler] = None,
        tool_registry: Optional[ToolRegistry] = None,
    ):
        self.agent = agent
        self.llm_service = llm_service or LLMService()
        self.context_assembler = context_assembler or ContextAssembler()
        self.tool_registry = tool_registry or ToolRegistry()
        
        # Execution state
        self.iteration = 0
        self.max_iterations = agent.agent_type.max_iterations
        self.is_cancelled = False
        
        # Load agent type handler
        self.agent_type_handler = self._load_agent_type_handler()
    
    def _load_agent_type_handler(self):
        """Load the appropriate agent type handler"""
        from apps.agents.types import get_agent_type_handler
        return get_agent_type_handler(self.agent.agent_type.role)
    
    async def execute_task(
        self,
        task: Task,
        stream: bool = True
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Execute a task with the agent.
        
        Yields execution events for streaming to client.
        """
        logger.info(f"Agent {self.agent.id} starting task {task.id}")
        
        # Update agent status
        self._update_agent_status(AgentStatus.WORKING, task)
        
        try:
            async for event in self._execution_loop(task, stream):
                yield event
                
                # Check for cancellation
                if self.is_cancelled:
                    yield {
                        "type": "cancelled",
                        "message": "Execution cancelled by user"
                    }
                    break
        
        except Exception as e:
            logger.exception(f"Error in agent execution: {e}")
            yield {
                "type": "error",
                "error": str(e)
            }
            self._update_agent_status(AgentStatus.ERROR)
            raise
        
        finally:
            # Save final state
            self._save_checkpoint()
    
    async def _execution_loop(
        self,
        task: Task,
        stream: bool
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """Main execution loop"""
        
        while self.iteration < self.max_iterations:
            self.iteration += 1
            
            yield {
                "type": "iteration_start",
                "iteration": self.iteration,
                "max_iterations": self.max_iterations
            }
            
            # 1. Assemble context
            context = await self._assemble_context(task)
            
            yield {
                "type": "context_assembled",
                "token_count": context.get("token_count", 0)
            }
            
            # 2. Generate system prompt
            system_prompt = self.agent_type_handler.get_system_prompt(context)
            
            # 3. Call LLM
            response = await self._call_llm(system_prompt, context, stream)
            
            # Yield thinking (for R1 models)
            if response.thinking:
                yield {
                    "type": "thinking",
                    "content": response.thinking
                }
            
            # 4. Check for final answer
            if response.is_final:
                yield {
                    "type": "final_response",
                    "content": response.message
                }
                
                # Complete the task
                await self._complete_task(task, response)
                
                yield {
                    "type": "task_completed",
                    "task_id": str(task.id),
                    "summary": response.message[:500]
                }
                return
            
            # 5. Handle user input request
            if response.needs_user_input:
                yield {
                    "type": "user_input_required",
                    "question": response.user_question
                }
                
                self._update_agent_status(AgentStatus.WAITING_INPUT)
                return
            
            # 6. Handle blocker
            if response.is_blocked:
                yield {
                    "type": "blocked",
                    "description": response.blocker_description
                }
                
                await self._handle_blocker(task, response.blocker_description)
                return
            
            # 7. Execute tool calls
            if response.tool_calls:
                for tool_call in response.tool_calls:
                    yield {
                        "type": "tool_call_start",
                        "tool": tool_call.tool_name,
                        "arguments": tool_call.arguments
                    }
                    
                    result = await self._execute_tool(tool_call, task)
                    
                    yield {
                        "type": "tool_call_result",
                        "tool": tool_call.tool_name,
                        "success": result.get("success", False),
                        "result": result
                    }
                    
                    # Add result to working memory
                    self._update_working_memory(tool_call, result)
            
            # 8. Yield agent message
            if response.message:
                yield {
                    "type": "agent_message",
                    "content": response.message
                }
            
            # 9. Check if we should continue
            if self._should_stop(response):
                break
            
            # Small delay to prevent tight loops
            await asyncio.sleep(0.1)
        
        # Max iterations reached
        yield {
            "type": "max_iterations",
            "message": f"Reached maximum iterations ({self.max_iterations})"
        }
        
        self._update_agent_status(AgentStatus.BLOCKED)
    
    async def _assemble_context(self, task: Task) -> Dict[str, Any]:
        """Assemble context for the agent"""
        return await self.context_assembler.assemble(
            agent=self.agent,
            task=task,
            project=self.agent.project
        )
    
    async def _call_llm(
        self,
        system_prompt: str,
        context: Dict[str, Any],
        stream: bool
    ) -> AgentResponse:
        """Call the LLM and parse response"""
        
        # Build messages
        messages = self._build_messages(context)
        
        # Get tool definitions
        tools = self.agent_type_handler.get_tool_definitions()
        
        # Call LLM
        if stream:
            response_text = ""
            async for chunk in self.llm_service.generate_stream(
                model=self.agent.model,
                system=system_prompt,
                messages=messages,
                tools=tools,
                temperature=self.agent.temperature
            ):
                response_text += chunk
        else:
            response_text = await self.llm_service.generate(
                model=self.agent.model,
                system=system_prompt,
                messages=messages,
                tools=tools,
                temperature=self.agent.temperature
            )
        
        # Parse response
        return self._parse_response(response_text)
    
    def _build_messages(self, context: Dict[str, Any]) -> list:
        """Build message history for LLM"""
        messages = []
        
        # Add conversation history
        for msg in self.agent.conversation_history[-20:]:  # Last 20 messages
            messages.append({
                "role": msg["role"],
                "content": msg["content"]
            })
        
        # Add current task context as user message
        if context.get("current_task_prompt"):
            messages.append({
                "role": "user",
                "content": context["current_task_prompt"]
            })
        
        return messages
    
    def _parse_response(self, response_text: str) -> AgentResponse:
        """Parse LLM response into structured AgentResponse"""
        from apps.llm.services.prompt_service import ResponseParser
        return ResponseParser.parse(response_text)
    
    async def _execute_tool(
        self,
        tool_call: ToolCall,
        task: Task
    ) -> Dict[str, Any]:
        """Execute a tool call"""
        
        # Get tool
        tool = self.tool_registry.get(tool_call.tool_name)
        if not tool:
            return {
                "success": False,
                "error": f"Unknown tool: {tool_call.tool_name}"
            }
        
        # Execute with context
        result = await tool.execute(
            arguments=tool_call.arguments,
            agent=self.agent,
            task=task,
            project=self.agent.project
        )
        
        # Log action
        self._log_action(tool_call, result, task)
        
        return result
    
    def _log_action(
        self,
        tool_call: ToolCall,
        result: Dict[str, Any],
        task: Task
    ):
        """Log agent action for event sourcing"""
        AgentAction.objects.create(
            project=self.agent.project,
            agent=self.agent,
            task=task,
            action_type=f"tool_{tool_call.tool_name}",
            action_data={
                "tool": tool_call.tool_name,
                "arguments": tool_call.arguments,
                "reasoning": tool_call.reasoning,
                "result": result
            },
            is_reversible=result.get("is_reversible", False),
            reverse_action=result.get("reverse_action")
        )
    
    def _update_working_memory(
        self,
        tool_call: ToolCall,
        result: Dict[str, Any]
    ):
        """Update agent's working memory with tool result"""
        memory = self.agent.working_memory
        
        # Track tool calls
        if "tool_calls" not in memory:
            memory["tool_calls"] = []
        
        memory["tool_calls"].append({
            "tool": tool_call.tool_name,
            "arguments": tool_call.arguments,
            "result_summary": result.get("summary", str(result)[:200]),
            "timestamp": datetime.now().isoformat()
        })
        
        # Keep last 20 tool calls
        memory["tool_calls"] = memory["tool_calls"][-20:]
        
        # Update files in progress
        if tool_call.tool_name in ["write_file", "modify_file", "create_file"]:
            if "files_in_progress" not in memory:
                memory["files_in_progress"] = []
            file_path = tool_call.arguments.get("file_path")
            if file_path and file_path not in memory["files_in_progress"]:
                memory["files_in_progress"].append(file_path)
        
        self.agent.working_memory = memory
        self.agent.save(update_fields=["working_memory"])
    
    def _update_agent_status(
        self,
        status: AgentStatus,
        task: Optional[Task] = None
    ):
        """Update agent status"""
        self.agent.status = status
        if task:
            self.agent.current_task = task
        self.agent.last_active_at = datetime.now()
        self.agent.save()
    
    async def _complete_task(self, task: Task, response: AgentResponse):
        """Mark task as completed"""
        task.status = TaskStatus.COMPLETED
        task.completion_summary = response.message
        task.completed_at = datetime.now()
        task.deliverables = response.files_created + response.files_modified
        task.save()
        
        # Update agent stats
        self.agent.tasks_completed += 1
        self.agent.status = AgentStatus.IDLE
        self.agent.current_task = None
        self.agent.save()
    
    async def _handle_blocker(self, task: Task, description: str):
        """Handle a blocked task"""
        task.status = TaskStatus.BLOCKED
        task.save()
        
        self.agent.status = AgentStatus.BLOCKED
        self.agent.status_message = description
        self.agent.save()
        
        # TODO: Notify supervisor agent
    
    def _should_stop(self, response: AgentResponse) -> bool:
        """Determine if execution should stop"""
        return (
            response.is_final or
            response.needs_user_input or
            response.is_blocked or
            self.is_cancelled
        )
    
    def _save_checkpoint(self):
        """Save execution checkpoint"""
        from apps.projects.models import ProjectCheckpoint
        
        ProjectCheckpoint.objects.create(
            project=self.agent.project,
            name=f"Auto checkpoint - {datetime.now().isoformat()}",
            state_snapshot={
                "agent_id": str(self.agent.id),
                "iteration": self.iteration,
                "working_memory": self.agent.working_memory,
                "conversation_history": self.agent.conversation_history
            },
            created_by=str(self.agent.id),
            is_auto=True
        )
    
    def cancel(self):
        """Cancel execution"""
        self.is_cancelled = True

Part 4: Context Management System
context/services/context_assembler.py
python"""
Context Assembler

Assembles comprehensive context for agent execution.
This is the most critical component for agent collaboration.
"""

import logging
from typing import Dict, Any, List, Optional
from dataclasses import dataclass

from django.db.models import Q
from pgvector.django import CosineDistance

from apps.agents.models import AgentInstance, AgentMessage, AgentAction
from apps.tasks.models import Task
from apps.projects.models import Project
from apps.code.models import CodeArtifact, ArtifactRegistry
from apps.context.models import Memory, Decision
from apps.llm.services.token_service import TokenCounter

logger = logging.getLogger(__name__)


@dataclass
class ContextBudget:
    """Token budget allocation for context sections"""
    
    system_prompt: int = 1000
    project_manifest: int = 2000
    architecture_decisions: int = 2000
    current_task: int = 1500
    relevant_code: int = 10000
    api_contracts: int = 2000
    agent_states: int = 1000
    recent_changes: int = 2000
    dependencies: int = 2000
    conversation: int = 3000
    semantic_memory: int = 2000
    
    @property
    def total(self) -> int:
        return sum([
            self.system_prompt,
            self.project_manifest,
            self.architecture_decisions,
            self.current_task,
            self.relevant_code,
            self.api_contracts,
            self.agent_states,
            self.recent_changes,
            self.dependencies,
            self.conversation,
            self.semantic_memory,
        ])


class ContextAssembler:
    """
    Assembles context for agent execution.
    
    The context is assembled in layers:
    1. Project Manifest (always included)
    2. Architecture Decisions (relevant to task)
    3. Current Task Details
    4. Relevant Code (semantic search)
    5. API Contracts
    6. Other Agents' Status
    7. Recent Changes
    8. Dependencies
    9. Conversation History
    10. Semantic Memory (retrieved)
    """
    
    def __init__(
        self,
        token_counter: Optional[TokenCounter] = None,
        budget: Optional[ContextBudget] = None
    ):
        self.token_counter = token_counter or TokenCounter()
        self.budget = budget or ContextBudget()
    
    async def assemble(
        self,
        agent: AgentInstance,
        task: Task,
        project: Project,
        max_tokens: int = 28000  # Leave room for response
    ) -> Dict[str, Any]:
        """
        Assemble comprehensive context for the agent.
        
        Returns a dictionary with all context sections and metadata.
        """
        context = {}
        total_tokens = 0
        
        # 1. Project Manifest (always include)
        project_manifest = self._get_project_manifest(project)
        context["project_manifest"] = project_manifest
        total_tokens += self.token_counter.count(project_manifest)
        
        # 2. Current Task
        task_context = self._get_task_context(task)
        context["current_task"] = task_context
        context["current_task_prompt"] = self._format_task_prompt(task)
        total_tokens += self.token_counter.count(str(task_context))
        
        # 3. Architecture Decisions
        decisions = await self._get_relevant_decisions(project, task)
        context["architecture_decisions"] = decisions
        total_tokens += self.token_counter.count(str(decisions))
        
        # 4. API Contracts
        contracts = self._get_api_contracts(project, task)
        context["api_contracts"] = contracts
        total_tokens += self.token_counter.count(str(contracts))
        
        # 5. Relevant Code (semantic search)
        remaining_for_code = min(
            self.budget.relevant_code,
            max_tokens - total_tokens - 5000  # Reserve for other sections
        )
        relevant_code = await self._get_relevant_code(project, task, remaining_for_code)
        context["relevant_files"] = relevant_code
        total_tokens += sum(
            self.token_counter.count(f["content"]) 
            for f in relevant_code
        )
        
        # 6. Agent States
        agent_states = self._get_agent_states(project, agent)
        context["agents"] = agent_states
        context["team_agents"] = self._get_team_agents(agent)
        total_tokens += self.token_counter.count(str(agent_states))
        
        # 7. Recent Changes
        recent_changes = self._get_recent_changes(project)
        context["recent_changes"] = recent_changes
        total_tokens += self.token_counter.count(str(recent_changes))
        
        # 8. Dependencies
        dependencies = await self._get_task_dependencies(task)
        context["dependencies"] = dependencies
        total_tokens += self.token_counter.count(str(dependencies))
        
        # 9. Conversation History
        conversation = self._get_conversation_history(agent)
        context["user_history"] = conversation
        total_tokens += self.token_counter.count(str(conversation))
        
        # 10. Semantic Memory
        remaining = max_tokens - total_tokens
        if remaining > 500:
            memories = await self._retrieve_memories(project, task, remaining)
            context["semantic_memory"] = memories
            total_tokens += self.token_counter.count(str(memories))
        
        # Add metadata
        context["token_count"] = total_tokens
        context["max_tokens"] = max_tokens
        context["tech_stack"] = project.manifest.get("tech_stack", {})
        
        logger.info(f"Context assembled: {total_tokens} tokens")
        
        return context
    
    def _get_project_manifest(self, project: Project) -> Dict[str, Any]:
        """Get structured project manifest"""
        manifest = project.manifest or {}
        
        return {
            "name": project.name,
            "description": project.description,
            "status": project.status,
            "requirements": manifest.get("requirements", {}),
            "domain_model": manifest.get("domain_model", {}),
            "tech_stack": manifest.get("tech_stack", {}),
            "architecture": manifest.get("architecture", {}),
        }
    
    def _get_task_context(self, task: Task) -> Dict[str, Any]:
        """Get current task details"""
        return {
            "id": str(task.id),
            "title": task.title,
            "type": task.task_type,
            "priority": task.priority,
            "status": task.status,
            "description": task.description,
            "requirements": task.requirements,
            "acceptance_criteria": task.acceptance_criteria,
            "deliverables": task.deliverables,
            "iteration": task.iteration_count,
            "parent_task": str(task.parent_task_id) if task.parent_task_id else None,
        }
    
    def _format_task_prompt(self, task: Task) -> str:
        """Format task as a prompt for the agent"""
        prompt = f"""## Your Current Task

**{task.title}**

Type: {task.task_type}
Priority: {task.priority}
Status: {task.status}

### Description
{task.description}

### Requirements
"""
        if task.requirements:
            for key, value in task.requirements.items():
                if isinstance(value, list):
                    prompt += f"\n**{key}:**\n"
                    for item in value:
                        prompt += f"- {item}\n"
                else:
                    prompt += f"- **{key}:** {value}\n"
        
        prompt += "\n### Acceptance Criteria\n"
        for criterion in task.acceptance_criteria:
            prompt += f"- [ ] {criterion}\n"
        
        if task.deliverables:
            prompt += "\n### Expected Deliverables\n"
            for deliverable in task.deliverables:
                prompt += f"- {deliverable}\n"
        
        return prompt
    
    async def _get_relevant_decisions(
        self,
        project: Project,
        task: Task
    ) -> List[Dict[str, Any]]:
        """Get architecture decisions relevant to the task"""
        # Get decisions that affect components related to the task
        decisions = Decision.objects.filter(
            project=project,
            status__in=['approved', 'implemented']
        )
        
        # If task has affected components, filter by them
        if task.requirements.get("affected_components"):
            components = task.requirements["affected_components"]
            decisions = decisions.filter(
                affected_components__overlap=components
            )
        
        # Also do semantic search
        if task.description:
            from apps.context.embeddings.embedder import Embedder
            embedder = Embedder()
            query_embedding = await embedder.embed(task.description)
            
            semantic_decisions = Decision.objects.filter(
                project=project
            ).order_by(
                CosineDistance('embedding', query_embedding)
            )[:5]
            
            # Combine
            decision_ids = set(decisions.values_list('id', flat=True))
            for d in semantic_decisions:
                if d.id not in decision_ids:
                    decisions = decisions | Decision.objects.filter(id=d.id)
        
        return [
            {
                "id": str(d.id),
                "type": d.decision_type,
                "title": d.title,
                "description": d.description,
                "reasoning": d.reasoning,
                "affected_components": d.affected_components,
            }
            for d in decisions[:10]  # Limit to 10
        ]
    
    def _get_api_contracts(
        self,
        project: Project,
        task: Task
    ) -> List[Dict[str, Any]]:
        """Get relevant API contracts"""
        # Get from artifact registry
        contracts = ArtifactRegistry.objects.filter(
            project=project,
            artifact_type='api_endpoint',
            is_active=True
        )
        
        return [
            {
                "name": c.name,
                "file_path": c.file_path,
                "details": c.details
            }
            for c in contracts[:20]
        ]
    
    async def _get_relevant_code(
        self,
        project: Project,
        task: Task,
        max_tokens: int
    ) -> List[Dict[str, Any]]:
        """Get code files relevant to the task via semantic search"""
        from apps.context.embeddings.embedder import Embedder
        embedder = Embedder()
        
        # Create query from task
        query = f"{task.title} {task.description}"
        if task.requirements.get("files_to_modify"):
            query += " " + " ".join(task.requirements["files_to_modify"])
        
        query_embedding = await embedder.embed(query)
        
        # Semantic search
        artifacts = CodeArtifact.objects.filter(
            project=project,
            embedding__isnull=False
        ).order_by(
            CosineDistance('embedding', query_embedding)
        )[:30]  # Get top 30, then filter by tokens
        
        # Also include explicitly mentioned files
        explicit_files = []
        if task.requirements.get("files_to_modify"):
            explicit = CodeArtifact.objects.filter(
                project=project,
                file_path__in=task.requirements["files_to_modify"]
            )
            explicit_files = list(explicit)
        
        # Combine and fit to token budget
        all_artifacts = explicit_files + [a for a in artifacts if a not in explicit_files]
        
        result = []
        tokens_used = 0
        
        for artifact in all_artifacts:
            artifact_tokens = self.token_counter.count(artifact.content)
            
            if tokens_used + artifact_tokens > max_tokens:
                # Try to include truncated version
                if tokens_used + 500 < max_tokens:
                    truncated = self._truncate_code(
                        artifact.content,
                        max_tokens - tokens_used - 100
                    )
                    result.append({
                        "path": artifact.file_path,
                        "language": artifact.language,
                        "content": truncated,
                        "truncated": True,
                        "structure": artifact.structure
                    })
                break
            
            result.append({
                "path": artifact.file_path,
                "language": artifact.language,
                "content": artifact.content,
                "truncated": False,
                "structure": artifact.structure
            })
            tokens_used += artifact_tokens
        
        return result
    
    def _truncate_code(self, content: str, max_tokens: int) -> str:
        """Truncate code while keeping important parts"""
        lines = content.split('\n')
        
        # Keep imports, class/function definitions
        important_lines = []
        for i, line in enumerate(lines):
            stripped = line.strip()
            if (
                stripped.startswith('import ') or
                stripped.startswith('from ') or
                stripped.startswith('class ') or
                stripped.startswith('def ') or
                stripped.startswith('async def ') or
                stripped.startswith('@')
            ):
                important_lines.append((i, line))
        
        # Build truncated version
        result_lines = []
        current_tokens = 0
        
        for idx, line in important_lines:
            line_tokens = self.token_counter.count(line)
            if current_tokens + line_tokens > max_tokens:
                break
            result_lines.append(line)
            current_tokens += line_tokens
        
        return '\n'.join(result_lines) + '\n\n# ... (truncated)'
    
    def _get_agent_states(
        self,
        project: Project,
        current_agent: AgentInstance
    ) -> List[Dict[str, Any]]:
        """Get status of all agents in the project"""
        agents = AgentInstance.objects.filter(
            project=project
        ).exclude(
            id=current_agent.id
        ).select_related('agent_type', 'current_task')
        
        return [
            {
                "id": str(a.id),
                "type": a.agent_type.name,
                "role": a.agent_type.role,
                "status": a.status,
                "current_task": a.current_task.title if a.current_task else None,
                "last_active": a.last_active_at.isoformat() if a.last_active_at else None
            }
            for a in agents
        ]
    
    def _get_team_agents(self, agent: AgentInstance) -> List[Dict[str, Any]]:
        """Get agents hired by this agent"""
        hired = agent.hired_agents.all().select_related('agent_type', 'current_task')
        
        return [
            {
                "id": str(a.id),
                "type": a.agent_type.name,
                "status": a.status,
                "current_task": a.current_task.title if a.current_task else None
            }
            for a in hired
        ]
    
    def _get_recent_changes(self, project: Project) -> List[Dict[str, Any]]:
        """Get recent changes in the project"""
        from datetime import timedelta
        from django.utils import timezone
        
        one_hour_ago = timezone.now() - timedelta(hours=1)
        
        actions = AgentAction.objects.filter(
            project=project,
            created_at__gte=one_hour_ago
        ).select_related('agent', 'agent__agent_type').order_by('-created_at')[:20]
        
        return [
            {
                "type": a.action_type,
                "agent": a.agent.agent_type.name,
                "data": a.action_data,
                "timestamp": a.created_at.isoformat()
            }
            for a in actions
        ]
    
    async def _get_task_dependencies(self, task: Task) -> Dict[str, Any]:
        """Get dependencies and their outputs"""
        deps = task.dependencies.select_related('depends_on')
        
        result = {
            "blocking": [],
            "outputs": []
        }
        
        for dep in deps:
            dep_task = dep.depends_on
            dep_info = {
                "task_id": str(dep_task.id),
                "title": dep_task.title,
                "status": dep_task.status,
                "type": dep.dependency_type
            }
            
            if dep_task.status == 'completed':
                # Include outputs
                dep_info["deliverables"] = dep_task.deliverables
                dep_info["summary"] = dep_task.completion_summary
                result["outputs"].append(dep_info)
            else:
                result["blocking"].append(dep_info)
        
        return result
    
    def _get_conversation_history(
        self,
        agent: AgentInstance,
        max_messages: int = 20
    ) -> List[Dict[str, str]]:
        """Get agent's conversation history"""
        return agent.conversation_history[-max_messages:]
    
    async def _retrieve_memories(
        self,
        project: Project,
        task: Task,
        max_tokens: int
    ) -> List[Dict[str, Any]]:
        """Retrieve relevant memories via semantic search"""
        from apps.context.embeddings.embedder import Embedder
        embedder = Embedder()
        
        query = f"{task.title} {task.description}"
        query_embedding = await embedder.embed(query)
        
        memories = Memory.objects.filter(
            project=project
        ).order_by(
            CosineDistance('embedding', query_embedding)
        )[:10]
        
        result = []
        tokens_used = 0
        
        for memory in memories:
            memory_tokens = self.token_counter.count(memory.content)
            if tokens_used + memory_tokens > max_tokens:
                break
            
            result.append({
                "type": memory.memory_type,
                "title": memory.title,
                "content": memory.content[:1000],  # Truncate individual memories
            })
            tokens_used += memory_tokens
            
            # Update access count
            memory.access_count += 1
            memory.save(update_fields=['access_count'])
        
        return result

Part 5: CLI Implementation
cli/devo_cli/main.py
python"""
DEVO CLI - Main Entry Point

An interactive CLI for building software with AI agents.
Similar to Claude Code but optimized for multi-agent collaboration.
"""

import typer
from rich.console import Console

from devo_cli.commands import init, chat, task, status, sync, agents, auth, config_cmd
from devo_cli.config import Config
from devo_cli.handlers.error_handler import setup_error_handling

# Create main app
app = typer.Typer(
    name="devo",
    help="DEVO - AI-Powered Development System",
    no_args_is_help=True,
)

# Console for rich output
console = Console()

# Register commands
app.add_typer(init.app, name="init", help="Initialize a new project")
app.add_typer(chat.app, name="chat", help="Interactive chat with AI agents")
app.add_typer(task.app, name="task", help="Manage tasks")
app.add_typer(status.app, name="status", help="View project status")
app.add_typer(sync.app, name="sync", help="Sync files with project")
app.add_typer(agents.app, name="agents", help="Manage agents")
app.add_typer(auth.app, name="auth", help="Authentication commands")
app.add_typer(config_cmd.app, name="config", help="Configuration")


@app.callback()
def main(
    ctx: typer.Context,
    verbose: bool = typer.Option(False, "--verbose", "-v", help="Verbose output"),
    config_file: str = typer.Option(None, "--config", "-c", help="Config file path"),
):
    """
    DEVO - Build software with AI agents.
    
    Get started:
    
        devo init                  # Initialize a new project
        devo chat                  # Start interactive session
        devo task "description"    # Assign a task
        devo status                # View project status
    """
    # Setup error handling
    setup_error_handling(verbose)
    
    # Load configuration
    ctx.ensure_object(dict)
    ctx.obj["config"] = Config.load(config_file)
    ctx.obj["verbose"] = verbose
    ctx.obj["console"] = console


@app.command()
def version():
    """Show version information"""
    from devo_cli import __version__
    console.print(f"DEVO CLI v{__version__}")


@app.command()
def doctor():
    """Check system requirements and configuration"""
    from devo_cli.utils.doctor import run_checks
    run_checks(console)


if __name__ == "__main__":
    app()
cli/devo_cli/commands/chat.py
python"""
Interactive Chat Command

Provides an interactive terminal interface for chatting with AI agents.
Supports:
- Streaming responses
- Interrupt handling (Ctrl+C)
- Multi-line input
- Agent status display
- File watching
"""

import asyncio
import signal
from typing import Optional

import typer
from rich.console import Console
from rich.live import Live
from rich.panel import Panel
from rich.markdown import Markdown
from rich.spinner import Spinner
from rich.prompt import Prompt

from devo_cli.api_client import APIClient
from devo_cli.ui.chat_interface import ChatInterface
from devo_cli.handlers.interrupt_handler import InterruptHandler
from devo_cli.state.session import Session

app = typer.Typer()
console = Console()


@app.callback(invoke_without_command=True)
def chat(
    ctx: typer.Context,
    project_id: Optional[str] = typer.Option(
        None, "--project", "-p",
        help="Project ID (uses current project if not specified)"
    ),
    resume: bool = typer.Option(
        False, "--resume", "-r",
        help="Resume from last checkpoint"
    ),
):
    """
    Start an interactive chat session with AI agents.
    
    This is the main interface for interacting with DEVO.
    You can describe what you want to build, ask questions,
    and watch agents work in real-time.
    
    Press Ctrl+C to interrupt and get options.
    """
    config = ctx.obj["config"]
    
    # Get or detect project
    project_id = project_id or config.get("current_project")
    if not project_id:
        console.print("[yellow]No project specified. Use 'devo init' to create one.[/]")
        raise typer.Exit(1)
    
    # Run chat loop
    asyncio.run(chat_loop(project_id, config, resume))


async def chat_loop(project_id: str, config: dict, resume: bool):
    """Main chat loop"""
    
    # Initialize components
    api_client = APIClient(config)
    session = Session(project_id)
    chat_ui = ChatInterface(console)
    interrupt_handler = InterruptHandler()
    
    # Load project info
    try:
        project = await api_client.get_project(project_id)
        console.print(Panel(
            f"[bold]{project['name']}[/]\n"
            f"Status: {project['status']}\n"
            f"Agents: {len(project.get('agents', []))} active",
            title="Project Loaded",
            border_style="green"
        ))
    except Exception as e:
        console.print(f"[red]Failed to load project: {e}[/]")
        return
    
    # Resume from checkpoint if requested
    if resume:
        checkpoint = session.load_checkpoint()
        if checkpoint:
            console.print("[dim]Resumed from checkpoint[/]")
            chat_ui.restore_history(checkpoint.get("history", []))
    
    # Print help
    console.print("\n[dim]Type your message and press Enter. "
                  "Use Ctrl+C to interrupt. Type 'exit' to quit.[/]\n")
    
    # Setup interrupt handler
    interrupt_handler.setup()
    
    # Main loop
    while True:
        try:
            # Get user input
            user_input = await chat_ui.get_input()
            
            if user_input.lower() in ['exit', 'quit', 'q']:
                # Save checkpoint before exit
                session.save_checkpoint({
                    "history": chat_ui.get_history()
                })
                console.print("[dim]Session saved. Goodbye![/]")
                break
            
            if user_input.lower() == 'status':
                await show_status(api_client, project_id)
                continue
            
            if user_input.lower() == 'agents':
                await show_agents(api_client, project_id)
                continue
            
            if user_input.lower() == 'help':
                show_help()
                continue
            
            # Send to API and stream response
            await process_message(
                api_client,
                project_id,
                user_input,
                chat_ui,
                interrupt_handler,
                session
            )
        
        except KeyboardInterrupt:
            # Handle interrupt
            action = await interrupt_handler.handle_interrupt(console)
            
            if action == "exit":
                session.save_checkpoint({"history": chat_ui.get_history()})
                console.print("[dim]Session saved. Goodbye![/]")
                break
            elif action == "cancel":
                await api_client.cancel_current_task(project_id)
                console.print("[yellow]Task cancelled[/]")
            elif action == "continue":
                continue
        
        except Exception as e:
            console.print(f"[red]Error: {e}[/]")
            if config.get("verbose"):
                console.print_exception()


async def process_message(
    api_client: APIClient,
    project_id: str,
    message: str,
    chat_ui: ChatInterface,
    interrupt_handler: InterruptHandler,
    session: Session
):
    """Process a user message and stream the response"""
    
    # Display user message
    chat_ui.display_user_message(message)
    
    # Stream response
    with Live(console=console, refresh_per_second=10) as live:
        current_agent = None
        current_content = ""
        thinking_content = ""
        
        async for event in api_client.chat_stream(project_id, message):
            # Check for interrupt
            if interrupt_handler.is_interrupted:
                live.stop()
                return
            
            event_type = event.get("type")
            
            if event_type == "agent_start":
                current_agent = event.get("agent")
                live.update(Panel(
                    f"[dim]Agent: {current_agent}[/]\n\n"
                    f"[yellow]Starting...[/]",
                    title="Agent Response",
                    border_style="blue"
                ))
            
            elif event_type == "thinking":
                thinking_content += event.get("content", "")
                live.update(Panel(
                    f"[dim]Agent: {current_agent}[/]\n\n"
                    f"[italic dim]Thinking: {thinking_content[-500:]}[/]",
                    title="Agent Thinking",
                    border_style="dim"
                ))
            
            elif event_type == "content":
                current_content += event.get("content", "")
                live.update(Panel(
                    Markdown(current_content),
                    title=f"Agent: {current_agent}",
                    border_style="blue"
                ))
            
            elif event_type == "tool_call_start":
                tool = event.get("tool")
                live.update(Panel(
                    f"[dim]Agent: {current_agent}[/]\n\n"
                    f"{current_content}\n\n"
                    f"[yellow]🔧 Using tool: {tool}[/]",
                    title=f"Agent: {current_agent}",
                    border_style="blue"
                ))
            
            elif event_type == "tool_call_result":
                tool = event.get("tool")
                success = event.get("success")
                status = "✅" if success else "❌"
                live.update(Panel(
                    f"[dim]Agent: {current_agent}[/]\n\n"
                    f"{current_content}\n\n"
                    f"{status} Tool {tool} completed",
                    title=f"Agent: {current_agent}",
                    border_style="blue"
                ))
            
            elif event_type == "file_created":
                file_path = event.get("path")
                current_content += f"\n\n📄 Created: `{file_path}`"
                live.update(Panel(
                    Markdown(current_content),
                    title=f"Agent: {current_agent}",
                    border_style="green"
                ))
            
            elif event_type == "file_modified":
                file_path = event.get("path")
                current_content += f"\n\n📝 Modified: `{file_path}`"
                live.update(Panel(
                    Markdown(current_content),
                    title=f"Agent: {current_agent}",
                    border_style="green"
                ))
            
            elif event_type == "user_input_required":
                live.stop()
                question = event.get("question")
                console.print(Panel(
                    question,
                    title="Agent needs input",
                    border_style="yellow"
                ))
                
                # Get user input
                answer = Prompt.ask("[yellow]Your answer[/]")
                
                # Send answer back
                async for response_event in api_client.send_answer(project_id, answer):
                    # Continue processing
                    pass
                
                return
            
            elif event_type == "task_completed":
                summary = event.get("summary", "Task completed")
                live.update(Panel(
                    Markdown(current_content + f"\n\n---\n\n✅ **Task Completed**\n\n{summary}"),
                    title=f"Agent: {current_agent}",
                    border_style="green"
                ))
            
            elif event_type == "error":
                error = event.get("error")
                live.update(Panel(
                    f"[red]Error: {error}[/]",
                    title="Error",
                    border_style="red"
                ))
            
            elif event_type == "done":
                break
        
        # Save to history
        chat_ui.add_to_history("user", message)
        chat_ui.add_to_history("assistant", current_content)


async def show_status(api_client: APIClient, project_id: str):
    """Show project status"""
    status = await api_client.get_project_status(project_id)
    
    console.print(Panel(
        f"[bold]{status['name']}[/]\n"
        f"Status: {status['status']}\n"
        f"Tasks: {status['task_counts']}\n"
        f"Files: {status['file_count']}",
        title="Project Status",
        border_style="blue"
    ))


async def show_agents(api_client: APIClient, project_id: str):
    """Show agent status"""
    agents = await api_client.get_agents(project_id)
    
    agent_lines = []
    for agent in agents:
        status_icon = {
            'working': '🔄',
            'idle': '💤',
            'blocked': '🚫',
            'error': '❌',
        }.get(agent['status'], '❓')
        
        task_info = f" - {agent['current_task']}" if agent.get('current_task') else ""
        agent_lines.append(
            f"{status_icon} [{agent['status']}] {agent['type']}{task_info}"
        )
    
    console.print(Panel(
        "\n".join(agent_lines) or "No active agents",
        title="Agents",
        border_style="blue"
    ))


def show_help():
    """Show help information"""
    console.print(Panel(
        """[bold]Chat Commands:[/]
        
    status    - Show project status
    agents    - Show agent status
    help      - Show this help
    exit      - Save and exit
    
[bold]Keyboard Shortcuts:[/]
    
    Ctrl+C    - Interrupt current operation
    Ctrl+D    - Exit (same as 'exit')
    
[bold]Tips:[/]
    
    - Describe what you want to build in plain language
    - Ask clarifying questions if agents ask
    - Use 'status' to check progress
    - Agents work autonomously, you can interrupt anytime""",
        title="Help",
        border_style="dim"
    ))
cli/devo_cli/handlers/interrupt_handler.py
python"""
Interrupt Handler

Handles Ctrl+C interrupts gracefully, providing options to:
- Pause and ask a question
- Cancel current task
- Continue processing
- Save checkpoint and exit
"""

import signal
import asyncio
from typing import Optional
from enum import Enum

from rich.console import Console
from rich.prompt import Prompt
from rich.panel import Panel


class InterruptAction(str, Enum):
    PAUSE = "pause"
    CANCEL = "cancel"
    CONTINUE = "continue"
    EXIT = "exit"


class InterruptHandler:
    """
    Handles keyboard interrupts during agent execution.
    
    Provides a graceful way to:
    - Pause execution to ask questions
    - Cancel current operation
    - Continue execution
    - Save and exit
    """
    
    def __init__(self):
        self.is_interrupted = False
        self._original_handler = None
    
    def setup(self):
        """Setup signal handlers"""
        self._original_handler = signal.getsignal(signal.SIGINT)
        signal.signal(signal.SIGINT, self._handle_signal)
    
    def cleanup(self):
        """Restore original signal handlers"""
        if self._original_handler:
            signal.signal(signal.SIGINT, self._original_handler)
    
    def _handle_signal(self, signum, frame):
        """Handle SIGINT signal"""
        self.is_interrupted = True
    
    def reset(self):
        """Reset interrupt flag"""
        self.is_interrupted = False
    
    async def handle_interrupt(self, console: Console) -> str:
        """
        Handle an interrupt by showing options to the user.
        
        Returns:
            The action to take: 'pause', 'cancel', 'continue', 'exit'
        """
        console.print()  # New line
        console.print(Panel(
            """What would you like to do?

[1] Pause and ask a question
[2] Cancel current task
[3] Continue processing
[4] Save checkpoint and exit""",
            title="⚠️ Interrupted",
            border_style="yellow"
        ))
        
        choice = Prompt.ask(
            "Choice",
            choices=["1", "2", "3", "4"],
            default="3"
        )
        
        # Reset interrupt flag
        self.reset()
        
        actions = {
            "1": "pause",
            "2": "cancel",
            "3": "continue",
            "4": "exit"
        }
        
        action = actions[choice]
        
        if action == "pause":
            # Get user question
            question = Prompt.ask("[yellow]Your question[/]")
            # This will be handled by the caller
            return "pause", question
        
        return action, None


class CheckpointManager:
    """
    Manages saving and restoring execution checkpoints.
    
    Checkpoints include:
    - Agent state
    - Task progress
    - Conversation history
    - Pending file changes
    """
    
    def __init__(self, project_id: str, storage_dir: str = ".devo"):
        self.project_id = project_id
        self.storage_dir = storage_dir
    
    def save(self, state: dict):
        """Save checkpoint to disk"""
        import json
        import os
        from datetime import datetime
        
        # Create storage directory
        os.makedirs(self.storage_dir, exist_ok=True)
        
        # Create checkpoint file
        checkpoint = {
            "project_id": self.project_id,
            "timestamp": datetime.now().isoformat(),
            "state": state
        }
        
        checkpoint_path = os.path.join(
            self.storage_dir,
            f"checkpoint_{self.project_id}.json"
        )
        
        with open(checkpoint_path, 'w') as f:
            json.dump(checkpoint, f, indent=2)
    
    def load(self) -> Optional[dict]:
        """Load checkpoint from disk"""
        import json
        import os
        
        checkpoint_path = os.path.join(
            self.storage_dir,
            f"checkpoint_{self.project_id}.json"
        )
        
        if not os.path.exists(checkpoint_path):
            return None
        
        with open(checkpoint_path, 'r') as f:
            checkpoint = json.load(f)
        
        return checkpoint.get("state")
    
    def clear(self):
        """Clear checkpoint"""
        import os
        
        checkpoint_path = os.path.join(
            self.storage_dir,
            f"checkpoint_{self.project_id}.json"
        )
        
        if os.path.exists(checkpoint_path):
            os.remove(checkpoint_path)

Part 6: LLM Integration
llm/clients/ollama.py
python"""
Ollama Client

Client for interacting with locally-running LLMs via Ollama.
Optimized for DeepSeek-R1 models.
"""

import json
import logging
import re
from typing import Dict, Any, List, Optional, AsyncGenerator
from dataclasses import dataclass

import aiohttp

logger = logging.getLogger(__name__)


@dataclass
class OllamaConfig:
    """Ollama configuration"""
    base_url: str = "http://localhost:11434"
    default_model: str = "deepseek-r1:7b"
    timeout: int = 300  # 5 minutes for long generations
    num_ctx: int = 32768  # Context window size


class OllamaClient:
    """
    Client for Ollama API.
    
    Supports:
    - Streaming and non-streaming generation
    - Tool/function calling (via prompt engineering)
    - DeepSeek R1 thinking extraction
    - Context window management
    """
    
    def __init__(self, config: Optional[OllamaConfig] = None):
        self.config = config or OllamaConfig()
        self._session: Optional[aiohttp.ClientSession] = None
    
    async def _get_session(self) -> aiohttp.ClientSession:
        """Get or create aiohttp session"""
        if self._session is None or self._session.closed:
            timeout = aiohttp.ClientTimeout(total=self.config.timeout)
            self._session = aiohttp.ClientSession(timeout=timeout)
        return self._session
    
    async def close(self):
        """Close the session"""
        if self._session and not self._session.closed:
            await self._session.close()
    
    async def generate(
        self,
        prompt: str,
        model: Optional[str] = None,
        system: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        tools: Optional[List[Dict]] = None,
    ) -> str:
        """
        Generate a response (non-streaming).
        
        Args:
            prompt: User prompt
            model: Model name (default: deepseek-r1:7b)
            system: System prompt
            temperature: Generation temperature
            max_tokens: Maximum tokens to generate
            tools: Tool definitions (will be added to prompt)
        
        Returns:
            Generated text
        """
        full_response = ""
        async for chunk in self.generate_stream(
            prompt=prompt,
            model=model,
            system=system,
            temperature=temperature,
            max_tokens=max_tokens,
            tools=tools,
        ):
            full_response += chunk
        
        return full_response
    
    async def generate_stream(
        self,
        prompt: str,
        model: Optional[str] = None,
        system: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        tools: Optional[List[Dict]] = None,
    ) -> AsyncGenerator[str, None]:
        """
        Generate a response with streaming.
        
        Yields chunks of generated text as they arrive.
        """
        model = model or self.config.default_model
        session = await self._get_session()
        
        # Build full prompt with tools if provided
        full_prompt = prompt
        if tools:
            full_prompt = self._add_tools_to_prompt(prompt, tools)
        
        # Build request payload
        payload = {
            "model": model,
            "prompt": full_prompt,
            "stream": True,
            "options": {
                "temperature": temperature,
                "num_ctx": self.config.num_ctx,
            }
        }
        
        if system:
            payload["system"] = system
        
        if max_tokens:
            payload["options"]["num_predict"] = max_tokens
        
        try:
            async with session.post(
                f"{self.config.base_url}/api/generate",
                json=payload
            ) as response:
                response.raise_for_status()
                
                async for line in response.content:
                    if line:
                        try:
                            data = json.loads(line)
                            if "response" in data:
                                yield data["response"]
                            if data.get("done"):
                                break
                        except json.JSONDecodeError:
                            continue
        
        except aiohttp.ClientError as e:
            logger.error(f"Ollama request failed: {e}")
            raise
    
    def _add_tools_to_prompt(
        self,
        prompt: str,
        tools: List[Dict]
    ) -> str:
        """
        Add tool definitions to the prompt.
        
        Since Ollama doesn't natively support function calling,
        we add tool definitions to the prompt and parse the response.
        """
        tools_section = """## Available Tools

You have access to the following tools. To use a tool, respond with a JSON block in this exact format:
```tool_call
{
    "tool": "tool_name",
    "arguments": {
        "arg1": "value1",
        "arg2": "value2"
    },
    "reasoning": "Brief explanation of why you're using this tool"
}
```

Available tools:

"""
        for tool in tools:
            tools_section += f"### {tool['name']}\n"
            tools_section += f"{tool['description']}\n"
            if tool.get('parameters'):
                tools_section += "Parameters:\n"
                for param_name, param_info in tool['parameters'].get('properties', {}).items():
                    required = param_name in tool['parameters'].get('required', [])
                    req_str = " (required)" if required else " (optional)"
                    tools_section += f"  - {param_name}{req_str}: {param_info.get('description', '')}\n"
            tools_section += "\n"
        
        tools_section += """
After using a tool, wait for the result before continuing.
If you need to use multiple tools, use them one at a time.
When you have completed the task, respond normally without a tool_call block.

---

"""
        return tools_section + prompt
    
    def parse_response(self, response: str) -> Dict[str, Any]:
        """
        Parse LLM response to extract thinking, tool calls, and answer.
        
        DeepSeek R1 outputs thinking in <think>...</think> tags.
        Tool calls are in ```tool_call...``` blocks.
        """
        result = {
            "thinking": "",
            "tool_calls": [],
            "answer": "",
            "is_final": True
        }
        
        # Extract thinking (R1 format)
        thinking_match = re.search(r"<think>(.*?)</think>", response, re.DOTALL)
        if thinking_match:
            result["thinking"] = thinking_match.group(1).strip()
            response = re.sub(r"<think>.*?</think>", "", response, flags=re.DOTALL)
        
        # Extract tool calls
        tool_call_pattern = r"```tool_call\s*\n?(.*?)\n?```"
        tool_matches = re.findall(tool_call_pattern, response, re.DOTALL)
        
        for match in tool_matches:
            try:
                tool_data = json.loads(match.strip())
                result["tool_calls"].append({
                    "tool": tool_data.get("tool"),
                    "arguments": tool_data.get("arguments", {}),
                    "reasoning": tool_data.get("reasoning", "")
                })
                result["is_final"] = False
            except json.JSONDecodeError:
                logger.warning(f"Failed to parse tool call: {match}")
        
        # Remove tool call blocks from answer
        answer = re.sub(tool_call_pattern, "", response, flags=re.DOTALL)
        result["answer"] = answer.strip()
        
        return result
    
    async def check_health(self) -> bool:
        """Check if Ollama is running and accessible"""
        try:
            session = await self._get_session()
            async with session.get(f"{self.config.base_url}/api/tags") as response:
                return response.status == 200
        except Exception:
            return False
    
    async def list_models(self) -> List[str]:
        """List available models"""
        session = await self._get_session()
        async with session.get(f"{self.config.base_url}/api/tags") as response:
            response.raise_for_status()
            data = await response.json()
            return [model["name"] for model in data.get("models", [])]
    
    async def pull_model(self, model: str) -> AsyncGenerator[Dict, None]:
        """Pull a model with progress"""
        session = await self._get_session()
        
        async with session.post(
            f"{self.config.base_url}/api/pull",
            json={"name": model, "stream": True}
        ) as response:
            async for line in response.content:
                if line:
                    try:
                        data = json.loads(line)
                        yield data
                    except json.JSONDecodeError:
                        continue

This completes the comprehensive specification for DEVO. The system is designed to be:

Highly collaborative - Agents share context through multiple layers
Interruptible - Users can stop at any time without losing progress
Recoverable - Checkpoints and event sourcing enable recovery
Local-first - Runs with DeepSeek-R1 on your machine
Production-ready - Proper error handling, logging, and testing
